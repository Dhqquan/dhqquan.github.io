<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Dhq&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.dhquan.cn/"/>
  <updated>2019-03-08T08:48:52.000Z</updated>
  <id>http://blog.dhquan.cn/</id>
  
  <author>
    <name>Dhq</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Neural Networks and Deep Learning Week4</title>
    <link href="http://blog.dhquan.cn/2019/03/08/Neural-Networks-and-Deep-Learning-Week4/"/>
    <id>http://blog.dhquan.cn/2019/03/08/Neural-Networks-and-Deep-Learning-Week4/</id>
    <published>2019-03-08T16:12:02.000Z</published>
    <updated>2019-03-08T08:48:52.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Neural-Networks-and-Deep-Learning-Week4"><a href="#Neural-Networks-and-Deep-Learning-Week4" class="headerlink" title="Neural Networks and Deep Learning Week4"></a>Neural Networks and Deep Learning Week4</h2><h4 id="Deep-L-layer-neural-network"><a href="#Deep-L-layer-neural-network" class="headerlink" title="Deep L-layer neural network"></a>Deep L-layer neural network</h4><p><img src="/images/Neural-Networks-and-Deep-Learning-Week4/image-20190308161637060.png" alt="image-20190308161637060"></p><p>这是一个四层神经网络。我们使用 $L$ 来表示层数。我们用 $n^{[l]}$ 表示第 $l$ 层上的单元数。</p><p>例如上图：$n^{[0]}=3,\ n^{[1]} = 5,\ n^{[2]}=5, \ n^{[3]}=3, \ n^{[4]}=1 $。</p><p>还使用 $a^{[l]}$ 表示第 $l$  层的激活函数。</p><a id="more"></a><p><br></p><h3 id="Forward-Propagation-in-a-Deep-Network"><a href="#Forward-Propagation-in-a-Deep-Network" class="headerlink" title="Forward Propagation in a Deep Network"></a>Forward Propagation in a Deep Network</h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Neural-Networks-and-Deep-Learning-Week4&quot;&gt;&lt;a href=&quot;#Neural-Networks-and-Deep-Learning-Week4&quot; class=&quot;headerlink&quot; title=&quot;Neural Networks and Deep Learning Week4&quot;&gt;&lt;/a&gt;Neural Networks and Deep Learning Week4&lt;/h2&gt;&lt;h4 id=&quot;Deep-L-layer-neural-network&quot;&gt;&lt;a href=&quot;#Deep-L-layer-neural-network&quot; class=&quot;headerlink&quot; title=&quot;Deep L-layer neural network&quot;&gt;&lt;/a&gt;Deep L-layer neural network&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;/images/Neural-Networks-and-Deep-Learning-Week4/image-20190308161637060.png&quot; alt=&quot;image-20190308161637060&quot;&gt;&lt;/p&gt;
&lt;p&gt;这是一个四层神经网络。我们使用 $L$ 来表示层数。我们用 $n^{[l]}$ 表示第 $l$ 层上的单元数。&lt;/p&gt;
&lt;p&gt;例如上图：$n^{[0]}=3,\ n^{[1]} = 5,\ n^{[2]}=5, \ n^{[3]}=3, \ n^{[4]}=1 $。&lt;/p&gt;
&lt;p&gt;还使用 $a^{[l]}$ 表示第 $l$  层的激活函数。&lt;/p&gt;
    
    </summary>
    
      <category term="deeplearning" scheme="http://blog.dhquan.cn/categories/deeplearning/"/>
    
      <category term="Neural Networks and Deep Learning" scheme="http://blog.dhquan.cn/categories/deeplearning/Neural-Networks-and-Deep-Learning/"/>
    
    
      <category term="笔记" scheme="http://blog.dhquan.cn/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="deeplearning" scheme="http://blog.dhquan.cn/tags/deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>Neural Networks and Deep Learning Week3</title>
    <link href="http://blog.dhquan.cn/2019/03/04/Neural-Networks-and-Deep-Learning-Week3/"/>
    <id>http://blog.dhquan.cn/2019/03/04/Neural-Networks-and-Deep-Learning-Week3/</id>
    <published>2019-03-04T10:51:05.000Z</published>
    <updated>2019-03-08T07:41:11.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Neural-Networks-and-Deep-Learning-Week3"><a href="#Neural-Networks-and-Deep-Learning-Week3" class="headerlink" title="Neural Networks and Deep Learning Week3"></a>Neural Networks and Deep Learning Week3</h2><h3 id="Neural-Network-Representation-神经网络的表示"><a href="#Neural-Network-Representation-神经网络的表示" class="headerlink" title="Neural Network Representation 神经网络的表示"></a>Neural Network Representation 神经网络的表示</h3><p><img src="/images/Neural-Networks-and-Deep-Learning-Week3/image-20190304105429049.png" alt="image-20190304105429049"></p><p>其中分成了输入层、隐藏层和输出层。</p><p>输入层：$x_1,  x_2, x_3, \cdots​$ </p><p>隐藏层：除了输入层和输出层之外的层</p><p>输入层：生成估计值 $\hat y​$ </p><p><br></p><a id="more"></a><p>在这里输入层表示为 $a^{[0]}$，$a^{[0]} = X$ 。</p><p>隐藏层表示为 $a^{[1]}$，在这里 $a^{[1]}$ 是一个 $4 \times 1​$ 的向量：</p><script type="math/tex; mode=display">a^{[1]} =  \begin{bmatrix}a_1^{[1]}\\ a_2^{[1]}\\a_3^{[1]}\\a_4^{[1]}\\\end{bmatrix}</script><p>$a^{[i]}_{j}$ 表示为第 $i$ 层中的第 $j$ 个神经元。</p><p>输出层表示为 $a^{[2]}$ ，$\hat y = a^{[2]}​$ 。</p><p><br></p><p>像上面这样的神经网网络，是一个 <strong>两层</strong> 的神经网络。我们在计算分层的时候，通常 <strong>忽略输入层</strong> ，所以在这里就是一个两层神经网络。</p><p><br></p><p><br></p><p>对于每一个神经单元来说，都有两步计算。第一步是计算 $z$ ，第二步是计算 $z$ 的激活函数 $sigmoid(z)$ 。</p><p><img src="/images/Neural-Networks-and-Deep-Learning-Week3/image-20190304120929941.png" alt="image-20190304120929941"></p><p>例如：计算 $a^{[1]}_1$</p><script type="math/tex; mode=display">z^{[1]}_1 = (w^{[1]}_{1})^{T} x+b^{[1]}_1\\a^{[1]}_1 = \sigma(z^{[1]}_1)\\</script><p>对于 $a^{[1]}_2 \cdots$ 来说也是一样。</p><script type="math/tex; mode=display">z^{[1]}_1 = (w^{[1]}_1)^{T} x+b^{[1]}_1, a^{[1]}_1 = \sigma(z^{[1]}_1)\\z^{[1]}_2 = (w^{[1]}_2)^{T} x+b^{[1]}_2, a^{[1]}_2 = \sigma(z^{[1]}_2)\\z^{[1]}_3 = (w^{[1]}_3)^{T} x+b^{[1]}_3, a^{[1]}_3 = \sigma(z^{[1]}_3)\\z^{[1]}_4 = (w^{[1]}_4)^{T} x+b^{[1]}_4, a^{[1]}_4 = \sigma(z^{[1]}_4)\\</script><h4 id="向量化："><a href="#向量化：" class="headerlink" title="向量化："></a>向量化：</h4><script type="math/tex; mode=display">z^{[1]}=\begin{bmatrix}\cdots(w_1^{[1]})^T\cdots\\\cdots(w_2^{[1]})^T\cdots\\\cdots(w_3^{[1]})^T\cdots\\\cdots(w_4^{[1]})^T\cdots\\\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\\\end{bmatrix}+\begin{bmatrix}b_1^{[1]}\\b_2^{[1]}\\b_3^{[1]}\\b_4^{[1]}\\\end{bmatrix}=\begin{bmatrix}(w^{[1]}_1)^{T} x+b^{[1]}_1\\(w^{[1]}_2)^{T} x+b^{[1]}_2\\(w^{[1]}_3)^{T} x+b^{[1]}_3\\(w^{[1]}_4)^{T} x+b^{[1]}_4\\\end{bmatrix}=\begin{bmatrix}z_1^{[1]}\\z_2^{[1]}\\z_3^{[1]}\\z_4^{[1]}\\\end{bmatrix}\\W^{[1]}=\begin{bmatrix}\cdots(w_1^{[1]})^T\cdots\\\cdots(w_2^{[1]})^T\cdots\\\cdots(w_3^{[1]})^T\cdots\\\cdots(w_4^{[1]})^T\cdots\\\end{bmatrix},W^{[1]} \in \mathbb{R}^{4\times 3}\\b^{[1]} = \begin{bmatrix}b_1^{[1]}\\b_2^{[1]}\\b_3^{[1]}\\b_4^{[1]}\\\end{bmatrix}, b^{[1]} \in \mathbb{R}^{4\times 1}\\</script><p>代码：</p><script type="math/tex; mode=display">\text{input X}, X \in \mathbb{R}^{3 \times 1} \\a^{[0]} = X\\z^{[1]} = W^{[1]}a^{[0]} + b^{[1]}\\a^{[1]} = \sigma(z^{[1]})\\z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}\\a^{[2]} = \sigma(z^{[2]})</script><p><br></p><h4 id="多组数据向量化"><a href="#多组数据向量化" class="headerlink" title="多组数据向量化"></a>多组数据向量化</h4><script type="math/tex; mode=display">Z^{[1]}=\begin{bmatrix}\cdots(w_1^{[1]})^T\cdots\\\cdots(w_2^{[1]})^T\cdots\\\cdots(w_3^{[1]})^T\cdots\\\cdots(w_4^{[1]})^T\cdots\\\end{bmatrix}\begin{bmatrix}x_1^{(1)} & x_1^{(2)} & \cdots & x_1^{(m)} \\x_2^{(1)} & x_2^{(2)} & \cdots & x_2^{(m)} \\x_3^{(1)} & x_3^{(2)} & \cdots & x_3^{(m)} \\\end{bmatrix}+\begin{bmatrix}b_1^{[1]}\\b_2^{[1]}\\b_3^{[1]}\\b_4^{[1]}\\\end{bmatrix}=\begin{bmatrix}\vdots & \vdots & \vdots & \vdots \\z^{[1](1)} & z^{[1](2)} & \cdots & z^{[1](m)}\\\vdots & \vdots & \vdots & \vdots \\\end{bmatrix}\\</script><p>代码：</p><script type="math/tex; mode=display">\text{input X}, X \in \mathbb{R}^{3 \times m} \\A^{[0]} = X\\Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]}\\A^{[1]} = \sigma(Z^{[1]})\\Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}\\A^{[2]} = \sigma(Z^{[2]})</script><p><br></p><h3 id="Activation-functions"><a href="#Activation-functions" class="headerlink" title="Activation functions"></a>Activation functions</h3><p> <img src="/images/Neural-Networks-and-Deep-Learning-Week3/image-20190307201508010.png" alt="image-20190307201508010"></p><p><br></p><h3 id="Derivatives-of-activation-functions"><a href="#Derivatives-of-activation-functions" class="headerlink" title="Derivatives of activation functions"></a>Derivatives of activation functions</h3><ul><li>$g(z) = \frac{1}{1+e^{-z}}$：$g’(z) = \frac{d}{dz}g(z) = g(z)(1-g(z))$ </li><li>$g(z) = tanh(z)$：$g’(z) = \frac{d}{dz}g(z) = 1 - g(z)^{2}$ </li><li>$g(z) = max(0, z)$ ： $g’(z) = \begin{cases}0 \ ,if \ z \lt 0 \\ 1\ ,if \ z \ge 0\\ \end{cases}$</li><li>$g(z) = max(0.01z, z)$ ： $g’(z) = \begin{cases}0.01 \ ,if \ z \lt 0 \\ 1\ ,if \ z \gt 0\\ \end{cases}$</li></ul><h3 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h3><p>如果将 $W$ 全部初始化为 $0$ 的话，每一个神经元都将会是一样的值，这样就从神经网络变成线性回归了。所以我们需要随机的初始化，同时这些值需要尽量的小，因为当 $z$ 很大的时候，梯度下降就会变得很慢。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Neural-Networks-and-Deep-Learning-Week3&quot;&gt;&lt;a href=&quot;#Neural-Networks-and-Deep-Learning-Week3&quot; class=&quot;headerlink&quot; title=&quot;Neural Networks and Deep Learning Week3&quot;&gt;&lt;/a&gt;Neural Networks and Deep Learning Week3&lt;/h2&gt;&lt;h3 id=&quot;Neural-Network-Representation-神经网络的表示&quot;&gt;&lt;a href=&quot;#Neural-Network-Representation-神经网络的表示&quot; class=&quot;headerlink&quot; title=&quot;Neural Network Representation 神经网络的表示&quot;&gt;&lt;/a&gt;Neural Network Representation 神经网络的表示&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/images/Neural-Networks-and-Deep-Learning-Week3/image-20190304105429049.png&quot; alt=&quot;image-20190304105429049&quot;&gt;&lt;/p&gt;
&lt;p&gt;其中分成了输入层、隐藏层和输出层。&lt;/p&gt;
&lt;p&gt;输入层：$x_1,  x_2, x_3, \cdots​$ &lt;/p&gt;
&lt;p&gt;隐藏层：除了输入层和输出层之外的层&lt;/p&gt;
&lt;p&gt;输入层：生成估计值 $\hat y​$ &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="deeplearning" scheme="http://blog.dhquan.cn/categories/deeplearning/"/>
    
      <category term="Neural Networks and Deep Learning" scheme="http://blog.dhquan.cn/categories/deeplearning/Neural-Networks-and-Deep-Learning/"/>
    
    
      <category term="笔记" scheme="http://blog.dhquan.cn/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="deeplearning" scheme="http://blog.dhquan.cn/tags/deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>Neural Networks and Deep Learning Week2</title>
    <link href="http://blog.dhquan.cn/2019/02/24/Neural-Networks-and-Deep-Learning-Week2/"/>
    <id>http://blog.dhquan.cn/2019/02/24/Neural-Networks-and-Deep-Learning-Week2/</id>
    <published>2019-02-24T14:59:04.000Z</published>
    <updated>2019-03-05T12:32:10.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Neural-Networks-and-Deep-Learning-Week2"><a href="#Neural-Networks-and-Deep-Learning-Week2" class="headerlink" title="Neural Networks and Deep Learning Week2"></a>Neural Networks and Deep Learning Week2</h2><h3 id="Logistic-Regression-as-a-Neural-Network"><a href="#Logistic-Regression-as-a-Neural-Network" class="headerlink" title="Logistic Regression as a Neural Network"></a>Logistic Regression as a Neural Network</h3><h4 id="Binary-Classification-二元分类"><a href="#Binary-Classification-二元分类" class="headerlink" title="Binary Classification 二元分类"></a>Binary Classification 二元分类</h4><p>在这里二分分类问题是：输入一张图片，能够判断出图片上的是否是猫，输出 1 说明是猫，输出 0 说明不是猫。</p><p><img src="/images/Neural-Networks-and-Deep-Learning-Week2/image-20190224150646148.png" alt="image-20190224150646148"></p><p>在计算机中，图片被分成<strong>红色</strong>、<strong>绿色</strong>和<strong>蓝色</strong>的三个矩阵。</p><p>我们使用特征向量来表示这个图片：</p><p><img src="/images/Neural-Networks-and-Deep-Learning-Week2/image-20190224151051158.png" alt="image-20190224151051158"></p><p>所以在这里 x 向量的维度是 64 * 64 * 3 = 12288。</p><a id="more"></a><p>每对输入 x 和 输出 y 有：</p><p>$(x,  y)$           $x \in \mathbb{R}^{n_x}, y \in \{0, 1\}$ 。</p><p>m 个训练集： $\{(x^1, y^1), (x^2, y^2), \dots, (x^m, y^m)\}$。</p><p>将所有的训练样本写成更加经凑，写成矩阵的形式：</p><script type="math/tex; mode=display">X = \begin{bmatrix}x^1 & x^2 & \cdots & x^m\\\end{bmatrix} ,x \in \mathbb{R}^{n_x}, X \in \mathbb{R}^{n_x \times m}\\Y = \begin{bmatrix}y^1 & y^2 & \cdots & y^m\end{bmatrix}, Y \in \mathbb{R}^{1 \times m}</script><p><br></p><h4 id="Logistic-Regression-逻辑回归"><a href="#Logistic-Regression-逻辑回归" class="headerlink" title="Logistic Regression 逻辑回归"></a>Logistic Regression 逻辑回归</h4><p>当给出一个表示图片的向量 $x$，算法将会计算出这张图片是猫的概率：</p><script type="math/tex; mode=display">\hat y = P(y = 1|x) \text{ where } 0 \le \hat y \le 1</script><p>我们使用线性函数 $ y = wx + b$，但是概率是在 $[0, 1]$ 之间的，所以我们使用了 <strong>sigmoid</strong> 函数来确保 $0 \le \hat y \le 1​$ 。</p><p>其中 <strong>sigmoid</strong> 函数是：</p><script type="math/tex; mode=display">\sigma(z) = \frac{1}{1+e^{-z}}</script><p><img src="/images/Neural-Networks-and-Deep-Learning-Week2/image-20190224160455020.png" alt="image-20190224160455020"></p><p>如果 z 很大，$e^{-z} \approx 0$，$\sigma(z) = \frac{1}{1+e^{-z}} \approx \frac{1}{1+0} = 1$ 。</p><p>如果 z 很小（负数），$e^{-z} \approx +\infty$，  $\sigma(z) = \frac{1}{1+e^{-z}} \approx \frac{1}{1+(+\infty)} \approx 0​$。</p><p>如果 z = 0，$\sigma(z) = \frac{1}{1 + 0}  = 1$。</p><p><br></p><p>逻辑回归中的参数：</p><ul><li>输入向量： $x \in \mathbb{R}^{n_x}​$</li><li>训练标签：$y \in \{0, 1\}​$</li><li>权重：$w \in \mathbb{R}^{n_x}$ </li><li>额外的特征：$b \in \mathbb{R}$</li><li>输出：$\hat y = \sigma(w^Tx+b)​$</li><li>sigmoid：$s = \sigma(z) = \sigma(w^Tx+b) = \frac{1}{1+e^{-z}} = \frac{1}{1+e^{-(w^T+b)}}$</li></ul><p><br></p><h4 id="Logistic-Regressioon-Cost-Function-逻辑回归代价函数"><a href="#Logistic-Regressioon-Cost-Function-逻辑回归代价函数" class="headerlink" title="Logistic Regressioon Cost Function 逻辑回归代价函数"></a>Logistic Regressioon Cost Function 逻辑回归代价函数</h4><p>为了训练参数 $w$ 和 $b$ ，我们需要定义代价函数。</p><h5 id="Lost-Function-损失函数"><a href="#Lost-Function-损失函数" class="headerlink" title="Lost Function 损失函数"></a>Lost Function 损失函数</h5><p>对于每个训练例子来说，我们计算 $\hat y^i$ 与 $y^i$ 之间的差异。其中计算差异的函数有很多，比如方差是其中一种，但是我们在这里并不是使用方差来计算（因为方差不能够算出全局最优解），所以我们使用下面的损失函数：</p><script type="math/tex; mode=display">L(\hat y^{(i)}, y^{(i)}) = -(y^{(i)}log(\hat y^{(i)})+(1-y^{(i)})log(1-\hat y^{(i)}))</script><ul><li>当 $y^{(i)}=1$：$L(\hat y^{(i)}, y^{(i)}) = -log(\hat y^{(i)})$ ，想要 $log(y^{i})$ 尽量的大，那么 $y^{(i)}$ 要尽量的大，$y^{(i)}$ 要无限趋近与 1。</li><li>当 $y^{(i)} = 0$：$L(\hat y^{(i)}, y^{(i)}) = -log(1-\hat y^{(i)})$，想要 $log(1-\hat y^{(i)})$ 尽量的大，那么 $\hat y^{(i)}$ 要尽量的小，$\hat y^{(i)}$ 要无限趋近与 0。</li></ul><h5 id="Cost-Function-代价函数"><a href="#Cost-Function-代价函数" class="headerlink" title="Cost Function 代价函数"></a>Cost Function 代价函数</h5><p>代价函数就是全部训练自己的损失函数之和。我们需要找到合适的 $w$ 和 $b$ 是的代价函数最小。</p><script type="math/tex; mode=display">J(w, b) = \frac{1}{m}\sum_{i=1}^{m}L(\hat y^{(i)},y^{(i)})=-\frac{1}{m}\sum_{i=1}^{m}\left[ y^{(i)}log(\hat y^{(i)})+(1-y^{(i)})log(1-\hat y^{(i)}) \right]</script><p><br></p><h4 id="Gradient-Descent-梯度下降"><a href="#Gradient-Descent-梯度下降" class="headerlink" title="Gradient Descent 梯度下降"></a>Gradient Descent 梯度下降</h4><p>使用梯度下降，找到 $w$ 和 $b$ 使得代价函数最小。</p><script type="math/tex; mode=display">w := w - \alpha \frac{dJ(w,b)}{dw}\\b := b - \alpha \frac{dJ(w,b)}{db}</script><p>其中 $\alpha$ 是学习速率，不能够太大和大小。</p><p><br></p><h4 id="Logistic-Regression-Grandient-Descent-，逻辑回归梯度下降"><a href="#Logistic-Regression-Grandient-Descent-，逻辑回归梯度下降" class="headerlink" title="Logistic Regression Grandient Descent ，逻辑回归梯度下降"></a>Logistic Regression Grandient Descent ，逻辑回归梯度下降</h4><p>使用计算图（Computation graph）来计算一个数据。</p><p><img src="/images/Neural-Networks-and-Deep-Learning-Week2/image-20190225183251255.png" alt="image-20190225183251255"></p><ol><li>$da = -\frac{y}{a} + \frac{1-y}{1-a}$</li><li>$dz = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} = (-\frac{y}{a} + \frac{1-y}{1-a}) \times a(1-a) = a - y$</li><li>$dw_1 = x_1 \cdot dz​$</li><li>$dw_2 = x_2 \cdot dz​$  </li><li>$db = dz​$</li></ol><p>所以在梯度下降中：</p><script type="math/tex; mode=display">w_1 := w_1 - \alpha dw_1 \\w_2 := w_2 - \alpha dw_2 \\b := b - \alpha db</script><p><br></p><h4 id="Gradient-Descent-on-m-Examples-m个数据的梯度下降"><a href="#Gradient-Descent-on-m-Examples-m个数据的梯度下降" class="headerlink" title="Gradient Descent on m Examples m个数据的梯度下降"></a>Gradient Descent on m Examples m个数据的梯度下降</h4><p>对于 $w_1$ 参数来说：</p><script type="math/tex; mode=display">\frac{\partial J(w, b)}{\partial w_1} = \frac{1}{M}\sum_{i=1}^m \frac{\partial L(a^{(i)},y^{(i)})}{\partial{w_1}}</script><p><br></p><p>对于m个数据来说，梯度下降的流程是：</p><p>$J = 0; dw_1 = 0; dw_2 = 0; \cdots ; dw_n = 0;db = 0$</p><p>$\text{For i:= 1 to m}$</p><p>&ensp;&ensp;&ensp;&ensp;$z^{(i)} = w^Tx^{(i)}+b​$</p><p>&ensp;&ensp;&ensp;&ensp;$a^{(i)}=\sigma(z^{(i)})​$</p><p>&ensp;&ensp;&ensp;&ensp;$J \text{ +=} -\left[ y^{(i)}loga^{(i)}+(1-y^{(i)})log(1-a^{(i)}) \right]$</p><p>&ensp;&ensp;&ensp;&ensp;$dz^{(i)}=a^{(i)} - y^{(i)}​$</p><p>&ensp;&ensp;&ensp;&ensp;$dw_1 \text{ += } x_1^{(i)}dz^{(i)}​$</p><p>&ensp;&ensp;&ensp;&ensp;$dw_2 \text{ += }x_2^{(i)}dz^{(i)}$</p><p>&ensp;&ensp;&ensp;&ensp;$\vdots​$</p><p>&ensp;&ensp;&ensp;&ensp;$dw_n \text{ += }x_n^{(i)}dz^{(i)}​$</p><p>&ensp;&ensp;&ensp;&ensp;$db \text{ += } dz^{(i)}​$</p><p>$J \text{ /= } m​$</p><p>$dw_1 \text{ /= } m; dw_2 \text{ /= } m; \cdots; dw_n \text{ /= } m; db \text{ /= }m​$</p><p>通过上述的方法，可以计算出一次梯度下降的 $dw_1, dw_2, \cdots, dw_n, db$。</p><script type="math/tex; mode=display">w_1 := w_1 - \alpha dw_1 \\w_2 := w_2 - \alpha dw_2 \\\vdots \\w_n := w_n - \alpha dw_n \\b := b - \alpha db</script><p>就需要多次的重复这样的步骤，直至下降到全局最低点。</p><p><br></p><h3 id="Python-and-Vectorization"><a href="#Python-and-Vectorization" class="headerlink" title="Python and Vectorization"></a>Python and Vectorization</h3><h4 id="Vectorization-向量化"><a href="#Vectorization-向量化" class="headerlink" title="Vectorization 向量化"></a>Vectorization 向量化</h4><p>在 python 中使用 numpy 包，实现向量化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">a = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">b = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line"></span><br><span class="line">tic = time.time()</span><br><span class="line">c = np.dot(a, b)</span><br><span class="line">toc = time.time()</span><br><span class="line"></span><br><span class="line">print(c)</span><br><span class="line">print(<span class="string">"Vectorized version: "</span>+ str(<span class="number">1000</span>*(toc-tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line">c = <span class="number">0</span></span><br><span class="line">tic = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">    c += a[i] * b[i]</span><br><span class="line">toc = time.time()</span><br><span class="line"></span><br><span class="line">print(c)</span><br><span class="line">print(<span class="string">"For loop: "</span> + str(<span class="number">1000</span>*(toc-tic)) + <span class="string">"ms"</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/Neural-Networks-and-Deep-Learning-Week2/image-20190227095434784.png" alt="image-20190227095434784"></p><p>使用向量化计算，计算的时间可以大幅度的提高。</p><p><br></p><h4 id="Vectorizing-Logistic-Regression’s-Gradient-逻辑回归梯度下降向量化"><a href="#Vectorizing-Logistic-Regression’s-Gradient-逻辑回归梯度下降向量化" class="headerlink" title="Vectorizing Logistic Regression’s Gradient 逻辑回归梯度下降向量化"></a>Vectorizing Logistic Regression’s Gradient 逻辑回归梯度下降向量化</h4><p>对于每一个 $dw_i$ 来说</p><p>$dw_1 \text{ += } x_1^{(i)}dz^{(i)}​$</p><p>$dw_2 \text{ += }x_2^{(i)}dz^{(i)}$</p><p>$\vdots​$</p><p>$dw_n \text{ += }x_n^{(i)}dz^{(i)}​$</p><p>其实是 $dw_1 = \frac{1}{m}\sum_{i=1}^{m} x_1^{(i)}dz^{(i)} $ 一直到 $dw_{nx} = \frac{1}{m}\sum_{i=1}^{m}x_{nx}^{(i)}dz^{(i)}​$  </p><script type="math/tex; mode=display">dW=\frac{1}{m}*\begin{bmatrix}x_1^{(1)} & x_1^{(2)} & \cdots & x_1^{(m)} \\\vdots & \vdots & \cdots & \vdots \\x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\\vdots & \vdots & \cdots & \vdots \\x_{nx}^{(1)} & x_{nx}^{(2)} & \cdots & x_{nx}^{(m)} \\\end{bmatrix}\begin{bmatrix}dz^{(1)} \\ dz^{(2)} \\ \vdots \\ dz^{(m)}\end{bmatrix}= \frac{1}{m}*\begin{bmatrix}dw_1 \\ dw_2 \\ \vdots \\ dw_{nx} \end{bmatrix}</script><p>在 Python 中表示为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dW = (<span class="number">1</span>/m)*np.dot(X,dZ.T)</span><br></pre></td></tr></table></figure><p><br></p><p>对于 $db$ 来说 ，$db =  \frac{1}{m} \sum_{i=1}^m dz^{(i)}$，</p><p>在 Python 中表示为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db = (<span class="number">1</span>/m)*np.sum(dZ)</span><br></pre></td></tr></table></figure><p><br></p><p>而对于 $z^{(i)} = w^Tx^{(i)}+b$ 来说，很容易的推断出</p><script type="math/tex; mode=display">Z = w^TX+b</script><p>在 Python 中表示为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T, X) + b</span><br></pre></td></tr></table></figure><p><br></p><p><br></p><p>综上使用向量化的逻辑回归梯度下降的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#一次梯度下降</span></span><br><span class="line">Z = np.dot(w.T, X) + b</span><br><span class="line">A = sigmoid(Z)</span><br><span class="line">dZ = A - Y</span><br><span class="line">dW = (<span class="number">1</span>/m) * np.dot(X, dZ.T)</span><br><span class="line">db = (<span class="number">1</span>/m) * np.sum(dZ)</span><br><span class="line">w -= alpha * dW</span><br><span class="line">b -= alpha * db</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Neural-Networks-and-Deep-Learning-Week2&quot;&gt;&lt;a href=&quot;#Neural-Networks-and-Deep-Learning-Week2&quot; class=&quot;headerlink&quot; title=&quot;Neural Networks and Deep Learning Week2&quot;&gt;&lt;/a&gt;Neural Networks and Deep Learning Week2&lt;/h2&gt;&lt;h3 id=&quot;Logistic-Regression-as-a-Neural-Network&quot;&gt;&lt;a href=&quot;#Logistic-Regression-as-a-Neural-Network&quot; class=&quot;headerlink&quot; title=&quot;Logistic Regression as a Neural Network&quot;&gt;&lt;/a&gt;Logistic Regression as a Neural Network&lt;/h3&gt;&lt;h4 id=&quot;Binary-Classification-二元分类&quot;&gt;&lt;a href=&quot;#Binary-Classification-二元分类&quot; class=&quot;headerlink&quot; title=&quot;Binary Classification 二元分类&quot;&gt;&lt;/a&gt;Binary Classification 二元分类&lt;/h4&gt;&lt;p&gt;在这里二分分类问题是：输入一张图片，能够判断出图片上的是否是猫，输出 1 说明是猫，输出 0 说明不是猫。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/Neural-Networks-and-Deep-Learning-Week2/image-20190224150646148.png&quot; alt=&quot;image-20190224150646148&quot;&gt;&lt;/p&gt;
&lt;p&gt;在计算机中，图片被分成&lt;strong&gt;红色&lt;/strong&gt;、&lt;strong&gt;绿色&lt;/strong&gt;和&lt;strong&gt;蓝色&lt;/strong&gt;的三个矩阵。&lt;/p&gt;
&lt;p&gt;我们使用特征向量来表示这个图片：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/Neural-Networks-and-Deep-Learning-Week2/image-20190224151051158.png&quot; alt=&quot;image-20190224151051158&quot;&gt;&lt;/p&gt;
&lt;p&gt;所以在这里 x 向量的维度是 64 * 64 * 3 = 12288。&lt;/p&gt;
    
    </summary>
    
      <category term="deeplearning" scheme="http://blog.dhquan.cn/categories/deeplearning/"/>
    
      <category term="Neural Networks and Deep Learning" scheme="http://blog.dhquan.cn/categories/deeplearning/Neural-Networks-and-Deep-Learning/"/>
    
    
      <category term="笔记" scheme="http://blog.dhquan.cn/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="deeplearning" scheme="http://blog.dhquan.cn/tags/deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>Neural Networks and Deep Learning Week1</title>
    <link href="http://blog.dhquan.cn/2019/02/21/Neural-Networks-and-Deep-Learning-Week1/"/>
    <id>http://blog.dhquan.cn/2019/02/21/Neural-Networks-and-Deep-Learning-Week1/</id>
    <published>2019-02-21T20:25:01.000Z</published>
    <updated>2019-03-05T12:32:01.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Neural-Networks-and-Deep-Learning-Week1"><a href="#Neural-Networks-and-Deep-Learning-Week1" class="headerlink" title="Neural Networks and Deep Learning Week1"></a>Neural Networks and Deep Learning Week1</h2><h3 id="What-is-neural-network"><a href="#What-is-neural-network" class="headerlink" title="What is neural network"></a>What is neural network</h3><h4 id="Single-neural-network-单神经网络"><a href="#Single-neural-network-单神经网络" class="headerlink" title="Single neural network 单神经网络"></a>Single neural network 单神经网络</h4><p>例如输入房子的大小，预测房子的价格。</p><p><img src="/images/Neural-Networks-and-Deep-Learning-Week1/image-20190221231004247.png" alt="image-20190221231004247"></p><p>输入一个值 $x$，根据神经元 neural，获得预测结果 $y​$。</p><p>只有一个 neural 的称为单神经网络。</p><a id="more"></a><p><br></p><h4 id="Multiple-neural-network-多神经网络"><a href="#Multiple-neural-network-多神经网络" class="headerlink" title="Multiple neural network 多神经网络"></a>Multiple neural network 多神经网络</h4><p>相对于单神经网络而言，多神经网络拥有多个输入 $x$，已经对应的多个隐藏单元（神经元 neural）。</p><p><img src="/images/Neural-Networks-and-Deep-Learning-Week1/image-20190221231137400.png" alt="image-20190221231137400"></p><p><br></p><h3 id="Supervised-learning-for-Neural-Network"><a href="#Supervised-learning-for-Neural-Network" class="headerlink" title="Supervised learning for Neural Network"></a>Supervised learning for Neural Network</h3><ul><li>Structured Data</li><li>Unstructured Data</li></ul><p><br></p><h3 id="Why-is-deep-learning-taking-off"><a href="#Why-is-deep-learning-taking-off" class="headerlink" title="Why is deep learning taking off"></a>Why is deep learning taking off</h3><ol><li>a large amount of data available</li><li>faster computation</li><li>innovation in the development of neural network algorithm</li></ol><p><br></p><p><img src="/images/Neural-Networks-and-Deep-Learning-Week1/image-20190222160026366.png" alt="image-20190222160026366"></p><p>To get high level of performance:</p><ol><li>Being able to train a big enough neural network</li><li>Huge amount of labeled data</li></ol><p>神经网络的训练是一个不断迭代的过程</p><p><img src="/images/Neural-Networks-and-Deep-Learning-Week1/image-20190222160345144.png" alt="image-20190222160345144"></p><p>越快的计算能力，就能够更好的进行迭代，提升算法。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Neural-Networks-and-Deep-Learning-Week1&quot;&gt;&lt;a href=&quot;#Neural-Networks-and-Deep-Learning-Week1&quot; class=&quot;headerlink&quot; title=&quot;Neural Networks and Deep Learning Week1&quot;&gt;&lt;/a&gt;Neural Networks and Deep Learning Week1&lt;/h2&gt;&lt;h3 id=&quot;What-is-neural-network&quot;&gt;&lt;a href=&quot;#What-is-neural-network&quot; class=&quot;headerlink&quot; title=&quot;What is neural network&quot;&gt;&lt;/a&gt;What is neural network&lt;/h3&gt;&lt;h4 id=&quot;Single-neural-network-单神经网络&quot;&gt;&lt;a href=&quot;#Single-neural-network-单神经网络&quot; class=&quot;headerlink&quot; title=&quot;Single neural network 单神经网络&quot;&gt;&lt;/a&gt;Single neural network 单神经网络&lt;/h4&gt;&lt;p&gt;例如输入房子的大小，预测房子的价格。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/Neural-Networks-and-Deep-Learning-Week1/image-20190221231004247.png&quot; alt=&quot;image-20190221231004247&quot;&gt;&lt;/p&gt;
&lt;p&gt;输入一个值 $x$，根据神经元 neural，获得预测结果 $y​$。&lt;/p&gt;
&lt;p&gt;只有一个 neural 的称为单神经网络。&lt;/p&gt;
    
    </summary>
    
      <category term="deeplearning" scheme="http://blog.dhquan.cn/categories/deeplearning/"/>
    
      <category term="Neural Networks and Deep Learning" scheme="http://blog.dhquan.cn/categories/deeplearning/Neural-Networks-and-Deep-Learning/"/>
    
    
      <category term="笔记" scheme="http://blog.dhquan.cn/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="deeplearning" scheme="http://blog.dhquan.cn/tags/deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>macOS 开启 NTFS 以及修复 NTFS 问题</title>
    <link href="http://blog.dhquan.cn/2019/02/20/macOS%E5%BC%80%E5%90%AF%E5%8E%9F%E5%A7%8BNTFS%E4%BB%A5%E5%8F%8A%E4%BF%AE%E5%A4%8DNTFS%E9%97%AE%E9%A2%98/"/>
    <id>http://blog.dhquan.cn/2019/02/20/macOS开启原始NTFS以及修复NTFS问题/</id>
    <published>2019-02-20T21:29:01.000Z</published>
    <updated>2019-03-05T12:28:55.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="macOS-开启-NTFS-以及修复-NTFS-问题"><a href="#macOS-开启-NTFS-以及修复-NTFS-问题" class="headerlink" title="macOS 开启 NTFS 以及修复 NTFS 问题"></a>macOS 开启 NTFS 以及修复 NTFS 问题</h2><p>由于装了黑苹果系统，为了更加方便的和 Windows10 系统传送文件。需要解决 macOS 中 NTFS 读写的问题。</p><h3 id="使用付费软件"><a href="#使用付费软件" class="headerlink" title="使用付费软件"></a>使用付费软件</h3><p>最简单的方法是使用网上的第三方付费软件。例如：Paragon NTFS for Mac、Tuxera NTFS for Mac。如果你是希捷硬盘的话，有免费的 Paragon NTFS for Mac 的希捷版，但是只能够是希捷的硬盘。</p><h3 id="macOS-原始-NTFS"><a href="#macOS-原始-NTFS" class="headerlink" title="macOS 原始 NTFS"></a>macOS 原始 NTFS</h3><p>macOS 本来是支持 NTFS 格式的硬盘的，但是由于微软的限制，把这个功能给隐藏了。</p><a id="more"></a><h4 id="1-查看磁盘的-Volume-Name"><a href="#1-查看磁盘的-Volume-Name" class="headerlink" title="1. 查看磁盘的 Volume Name"></a>1. 查看磁盘的 Volume Name</h4><p>打开终端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> diskutil list</span><br></pre></td></tr></table></figure><p>电脑里面的磁盘的 Volume Name 是 D。</p><h4 id="2-修改-etc-fstab-文件"><a href="#2-修改-etc-fstab-文件" class="headerlink" title="2. 修改 /etc/fstab 文件"></a>2. 修改 /etc/fstab 文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo vim /etc/fstab</span><br></pre></td></tr></table></figure><p>在 fstab 文件中写入 <code>LABEL=D none ntfs rw,auto,nobrowse</code>，如果有多个 ntfs 磁盘就一行写一个。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LABEL=D none ntfs rw,auto,nobrowse</span><br><span class="line">LABEL=E none ntfs rw,auto,nobrowse</span><br></pre></td></tr></table></figure><p>保存 fstab 文件。</p><h4 id="3-重新挂载硬盘"><a href="#3-重新挂载硬盘" class="headerlink" title="3. 重新挂载硬盘"></a>3. 重新挂载硬盘</h4><p>使用 macOS 的磁盘管理或者重启电脑。</p><h4 id="4-创建快捷方式"><a href="#4-创建快捷方式" class="headerlink" title="4. 创建快捷方式"></a>4. 创建快捷方式</h4><p>重新挂载完硬盘之后，你会发现桌面上没有显示磁盘了，因为在 fstab 中加入了 <code>nobrowse</code>，如果不加的话是无法读写 NTFS 的。</p><p>我们只需要在桌面上创建软连接（快捷方式）就可以了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo ln -s /Volumes/D ~/Desktop/D</span><br><span class="line"><span class="meta">$</span> sudo ln -s /Volumes/E ~/Desktop/E</span><br></pre></td></tr></table></figure><h3 id="macOS-下的-NTFS-问题"><a href="#macOS-下的-NTFS-问题" class="headerlink" title="macOS 下的 NTFS 问题"></a>macOS 下的 NTFS 问题</h3><p>在 macOS 下我会出现无法自动挂载 NTFS 可读写硬盘，既上述描述的方法失效了。但是如果删除 fstab 中的对 ntfs 磁盘开启读写的话，是可以重新挂载上只读。但是在 Windows10 下是没有什么问题的。在 Windows10 下出现了磁盘错误。</p><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p><del>在 Windows10 下修复磁盘错误。</del></p><p><del>1. 运行管理员权限的PoweShell</del><br><del>2. 输入命令 <code>chkdsk d:/f</code> 修复磁盘。如果需要修复多块磁盘可以修改盘符，例如：<code>chkdsk e:/f</code>。</del></p><p>2019-03-05 更新：</p><p>今天又模型其妙出现了 BUG ，而且还修复不好。</p><p>还是去用 Paragon NTFS for Mac 15 了</p><p><br></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://juejin.im/entry/59546970f265da6c34332bdd" target="_blank" rel="noopener">打开Mac OSX原生的读写NTFS功能</a></p><p><a href="https://jingyan.baidu.com/article/11c17a2c202d3df446e39d27.html" target="_blank" rel="noopener">Win10如何检查磁盘错误怎么修复驱动器错误</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;macOS-开启-NTFS-以及修复-NTFS-问题&quot;&gt;&lt;a href=&quot;#macOS-开启-NTFS-以及修复-NTFS-问题&quot; class=&quot;headerlink&quot; title=&quot;macOS 开启 NTFS 以及修复 NTFS 问题&quot;&gt;&lt;/a&gt;macOS 开启 NTFS 以及修复 NTFS 问题&lt;/h2&gt;&lt;p&gt;由于装了黑苹果系统，为了更加方便的和 Windows10 系统传送文件。需要解决 macOS 中 NTFS 读写的问题。&lt;/p&gt;
&lt;h3 id=&quot;使用付费软件&quot;&gt;&lt;a href=&quot;#使用付费软件&quot; class=&quot;headerlink&quot; title=&quot;使用付费软件&quot;&gt;&lt;/a&gt;使用付费软件&lt;/h3&gt;&lt;p&gt;最简单的方法是使用网上的第三方付费软件。例如：Paragon NTFS for Mac、Tuxera NTFS for Mac。如果你是希捷硬盘的话，有免费的 Paragon NTFS for Mac 的希捷版，但是只能够是希捷的硬盘。&lt;/p&gt;
&lt;h3 id=&quot;macOS-原始-NTFS&quot;&gt;&lt;a href=&quot;#macOS-原始-NTFS&quot; class=&quot;headerlink&quot; title=&quot;macOS 原始 NTFS&quot;&gt;&lt;/a&gt;macOS 原始 NTFS&lt;/h3&gt;&lt;p&gt;macOS 本来是支持 NTFS 格式的硬盘的，但是由于微软的限制，把这个功能给隐藏了。&lt;/p&gt;
    
    </summary>
    
      <category term="黑苹果" scheme="http://blog.dhquan.cn/categories/%E9%BB%91%E8%8B%B9%E6%9E%9C/"/>
    
      <category term="macOS" scheme="http://blog.dhquan.cn/categories/macOS/"/>
    
    
      <category term="macOS" scheme="http://blog.dhquan.cn/tags/macOS/"/>
    
      <category term="NTFS" scheme="http://blog.dhquan.cn/tags/NTFS/"/>
    
      <category term="黑苹果" scheme="http://blog.dhquan.cn/tags/%E9%BB%91%E8%8B%B9%E6%9E%9C/"/>
    
  </entry>
  
  <entry>
    <title>【论文翻译】Topical Word Embeddings</title>
    <link href="http://blog.dhquan.cn/2019/02/12/%E3%80%90%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E3%80%91Topical-Word-Embeddings/"/>
    <id>http://blog.dhquan.cn/2019/02/12/【论文翻译】Topical-Word-Embeddings/</id>
    <published>2019-02-12T14:53:57.000Z</published>
    <updated>2019-02-19T13:16:27.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Topical-Word-Embeddings"><a href="#Topical-Word-Embeddings" class="headerlink" title="Topical Word Embeddings"></a>Topical Word Embeddings</h2><p>Yang Liu, Zhiyuan Liu, Tat-Seng Chua, µ Sun</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>大多数的词嵌入模型通常使用单个向量来表示每一个单词，这使得这些模型在同音异义和一词多义上无法区分。为了增强辨别力，我们采用潜在主题模型（latent topic model）为语料库里的每一个词分配主题，并且基于词和主题来学习<strong>主题词嵌入（TWE）</strong>。这样能够灵活地获得语境词嵌入（contextual word embedding），来衡量语境中词的相似性。我们还可以构建文档向量表示（document representation），相比广泛使用的文档模型（如潜主题模型）更具表现力。在实验中，我们评估了 TWE 模型的两个任务：情景词相似性和文本分类。实验结果表明，我们的模型比经典的词嵌模型（包括基于语境相似度多种原型版本）表现好，同时在文本分类上超过了潜在主题模型和其他代表性的文档模型。本文的源代码能够在<a href="https://github.com/largelymfs/topical_word_embeddings" target="_blank" rel="noopener">https://github.com/largelymfs/topical_word_embeddings</a>。</p><a id="more"></a><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>词嵌入（word embedding）也被成为词表示（word representation），在基于语料库的上下文构建连续词向量中起着越来越重要的角色。词嵌入既能捕捉单词的语义和句法信息，又能用来测量词的相似性，因此词嵌入广泛应用于IR和NLP任务中。</p><p>大多数的词嵌入方法都是假定每一个词维护单个向量，但是无法解决一词多义和同音异义。多原型向量空间模型（Reisinger and Mooney 2010）提出将一个单词的上下文聚类成不同的群（group），然后每一个群生成不同的原型向量。遵循着这个想法，（Huang et al. 2012）提出了基于神经语言模型（Bengio et al. 2003）的多原型词嵌入。</p><p>尽管他们的模型游泳，但是多原型词嵌入面临以下几个挑战：(1) 这些模型为每一个词生成孤立的多原型向量，忽视了词之间复杂的相关性和和它们的上下文。(2) 在多原型设置中，一个单词的上下文被分成了没有重叠的群。而实际上，一个词的多个意思可能是互相关联的，它们之间没有明确的语义边界。</p><p>在这篇论文中，我们提出了一个更灵活、更强有力的多原型词嵌入框架——<strong>主题词嵌入（ TWE ）</strong>，其中<strong>主题词</strong>是指上下文主题的词。TWE 的基本思想是，我们允许每一个词在不同的主题下拥有不用的嵌入向量。例如，<em>苹果</em> 这个词，在 <em>食物</em> 的主题下代表是一种水果，在 <em>IT</em> 主题下代表一个 IT 公司。</p><p>我们使用 latent Dirichlet allocation ( LDA ) (Blei, Ng, and Jordan 2003 ) 来获得词主题。使用 collapsed Gibbs sampling ( Griffiths and Steyvers 2004 ) 来迭代地为每一个单词标记（ word token ）分配潜在的主题。 通过这个方法，给出一系列的单词 $D = \{ w_1, …, w_M \}$，经过 LDA 收敛，每一个单词标记 $w_i$ 会分配进一个具体的主题 $z_i$， 组成一个<strong>词-主题</strong>对 $\langle w_i, z_i \rangle$，用来学习主题词嵌入。我们设计了三种 TWE 模型来学习主题词向量，如 图1 所示，其中窗口大小是 1，$w_{i-1}$ 和 $w_{i+1}$ 是词 $w_i$ 的上下文。</p><p><strong>TWE-1</strong>. 我们将每一个主题视为一个伪词（pseudo word），分别学习主题向量和词向量。然后我们根据向量 $w_i$ 和 $z_i$  建立主题词向量 $\langle w_i, z_i \rangle$ 。</p><p><strong>TWE-2</strong>. 我们将每一个词-主题对 $\langle w_i, z_i \rangle$ 视为一个伪词，直接地学习主题词向量。</p><p><strong>TWE-3</strong>. 我们为每一个词和每一个主题保留不同嵌入向量。 通过将相关的词和主题嵌入向量联系起来，建立词-主题对的嵌入向量。</p><p>在三种 TWE 模型中，TWE-1 没有考虑词和它指定学习的主题之间的相互联系。TWE-2 通过简单地将词-主题对视为伪词，考虑了内在的联系，但是它存在稀疏性问题，因为每一个出现的词语都被严格地分配到不同的主题。相比于 TWE-2，TWE-3 则是在鉴别单词和稀疏性之间进行了权衡。但是在 TWE-3 学习过程中，主题嵌入向量会影响对应的单词嵌入向量，这可能导致这些词汇在相同的主题中不易辨别。</p><p>为了实现我们的 TWE 模型，我们使用了目前最好的词嵌入模型——SkipGram（Mikolov et al. 2013）。下一部分会详细的介绍三种 TWE 模型。 TWE 模型能够被用来计算给定上下文的语境词嵌入，并且能够用来表示一个从所有词的主题词嵌入聚合成为的文档。</p><p>我们在两个任务上测试了我们提出的模型，语境词相似和文本分类来测试我们的模型。实验结果表明，我们的模型比传统的和其他多原型词嵌入模型在语境词相似表现好，而且在文本分类上超过了广泛使用的基于主题和基于词嵌入的文档模型。</p><p>主要的贡献是我们将主题融入到基本的词嵌入表示中，并允许产生的主题词嵌入在不同的语境下获得一个词的不同含义。相比于多原型词嵌入模型分别为每一个词建立多原型嵌入，我们的模型利用主题模型去利用全部词和它们的上下文一起学习主题词向量。</p><h3 id="Our-Models"><a href="#Our-Models" class="headerlink" title="Our Models"></a>Our Models</h3><h4 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h4><p>Skip-Gram 是一个众所周知的学习词向量的框架（Mikkolov et al. 2013），如图1(A) 所示。Skip-Gram 目的是预测滑动窗口中给定目标单词的上下文单词。在这个框架中，每一个单词对应一个独一无二的向量，目标词的向量用来作为预测上下文单词的特征。</p><p>给定一个词序列 $D =  \{w_1,…, w_M\}$ ，Skip-Gram 的目标是最大化平均对数概率</p><script type="math/tex; mode=display">\mathcal{L}(D) = \frac{1}{M}\sum_{i=1}^{M}\sum_{-k\leq c\leq k, c\neq 0}logPr(w_{i+c}|wi)。\tag{1}</script><p>$k$ 是目标单词的上下文大小。 Skip-Gram 使用 softmax 函数计算 $Pr(w_c|wi)$:</p><script type="math/tex; mode=display">Pr(w_c|w_i) = \frac{exp(\textbf{w}_c\cdot \textbf{w}_i)}{\sum_{w_i\in W}exp(\textbf{w}_c\cdot \textbf{w}_i)},\tag{2}</script><p>$\textbf{w}_i$ 和 $\textbf{w}_c$ 分别表示目标单词 $w_i$ 和 上下文单词 $w_c$ 向量表示。$W$ 是单词词汇表。为了是模型更加高效的学习，我们使用了<strong>分层 softmax</strong> 和<strong>负采样</strong>技术。</p><p>通过 Skip-Gram 学习到的词向量可以用来计算词语的相似度。两个词与 $w_i​$ 和 $w_j​$ 的相似度可以用他们之间的向量内积度量 $S(w_i, w_j) = \textbf{w}_i \cdot \textbf{w}_j​$。在之前的工作中，单词相似度任务通常被用于评估词向量方法的表现（Mikolov et al. 2013; Baroni, Dinu, and Kruszewski 2014）。</p><p>正如 Introduction 部分所说的那要，我们希望通过引入潜在主题模型来提高词嵌入的表达能力。在 latent Dirichlet allocation(LDA) 作用下，根据概率 $Pr(z_i|w_i, d) \propto Pr(w_i|z_i)Pr(z_i|d)​$，我们为每一个词 $w_i​$ 分配了一个潜在主题 $z_i \in T​$。然后我们提出了三种主题词嵌入（TWE）。</p><h4 id="TWE-1"><a href="#TWE-1" class="headerlink" title="TWE-1"></a>TWE-1</h4><p>TWE-1 目标是分别同时地学习单词和主题的向量表达。对于每一个目标单词以及它的主题 $\langle w_i, z_i\rangle$， TWE-1 的目标是最大化平均对数概率：</p><script type="math/tex; mode=display">\mathcal{L}(D)=\frac{1}{M}\sum_{i=1}^{M}\sum_{-k\leq c\leq k,c\neq 0}logPr(w_{i+c}|w_i)+logPr(w_{i+c}|z_i).\tag{3}</script><p>相比于在 Skip-Gram 仅仅使用目标单词 $w_i​$ 来预上下文，TWE-1 还使用了目标单词的主题 $z_i​$ 来预测上下文。TWE-1 的基本方法是将每一个主题视为一个伪词，这个伪词出现在所有分配了这个主题的单词的位置上面。因此，主题的向量将表示这个主题下单词的集体的语义。</p><p>在 TWE-1 中，通过连接单词 $w$ 和主题 $z$ 获得了这个主题下的主题词嵌入，例如 $\textbf{w}^z=\textbf{w} \oplus \textbf{z}$，其中 $\oplus$ 是连接运算符，而且 $\textbf{w}^z$ 的长度是 $\textbf{w}$ 或 $\textbf{z}$ 的两倍。</p><h5 id="Contextual-Word-Embedding"><a href="#Contextual-Word-Embedding" class="headerlink" title="Contextual Word Embedding"></a>Contextual Word Embedding</h5><p>TWE-1 能够用来获得上下文的词嵌入。对于每一个词 $w$ 和它的上下文 $c$，TWE-1 首先通过将 $c $ 作为文档推断出主题贡献 $Pr(z|w,c)$，称为 $Pr(z|w,c) \propto Pr(w|z)Pr(z|c)$。通过主题分布，我们可以进一步的在 $c$ 主题 $w$ 词获得上下文词嵌入</p><script type="math/tex; mode=display">\textbf{w}^c=\sum_{z\in T}Pr(z|w,c)\textbf{w}^z,\tag{4}</script><p>$\textbf{w}^z$ 是 $c$ 主题下词 $w$ 的词嵌入，通过连接词向量 $\textbf{w}$ 和主题向量 $\textbf{z}$ 。</p><p>情景词嵌入将会哦用于计算上下文的单词相似度。给定一个词对和它们的上下文，即 $(w_i,c_i)$ 和 $(w_j, c_j)$，情景词的目的是计算这两个词{的相似性，可以表示为 $S(w_i,c_i,w_j,c_j)=(\textbf{w}_{i}^{c_i}\cdot \textbf{w}_{j}^{c_j})$，还可以表示为</p><script type="math/tex; mode=display">\sum_{z\in T}\sum_{z'\in T}Pr(z|w_i,c_i)Pr(z'|w_j,c_j)S(\textbf{w}^z,\textbf{w}^{z'}),\tag{5}</script><p>$S(\textbf{w}^z,\textbf{w}^{z’})​$ 是 $\textbf{w}^{z}​$ 和 $\textbf{w}^{z’}​$ 的相关性，本文使用 <strong>cos</strong> 相似度来计算。Eq.(5) 中的相似函数称为 AvgSimC（Reisinger and Mooney 2010）。</p><p>我们定义 MaxSimC (Reisinger and Mooney 2010) 作为 AvgSimC的代替。MaxSimC 选择最有可能的主题 $z$ 对应的主题词嵌入 $\textbf{w}^z$ ，推断在上下文 $c$ 中使用 $w$ 作为情景词嵌入。我们定义了 $w$ 词在上下文 $c$ 情景词嵌入</p><script type="math/tex; mode=display">\textbf{w}^c=\textbf{w}^z, z=argmax_zPr(z|w,c),\tag{6}</script><p>上下文词相似定义为</p><script type="math/tex; mode=display">S(w_i,c_i,w_j,c_j)=\textbf{w}_i^{z}\cdot \textbf{w}_j^{z'},\tag{7}</script><p>其中$z=argmax_zPr(z|w_i,c_i), z’=argmax_zPr(z|w_j,c_j)​$ .</p><h5 id="Document-Embedding"><a href="#Document-Embedding" class="headerlink" title="Document Embedding"></a>Document Embedding</h5><p>在 TWE-1 中，我们通过聚积每个词的主题词嵌入来表示语义字，例如：$\textbf{d}=\sum_{w\in d}Pr(w|d)\textbf{w}^z​$，其中 $Pr(w|d)​$ 能够加权 TFIDF 分数在 $d​$ 中。</p><h4 id="TWE-2"><a href="#TWE-2" class="headerlink" title="TWE-2"></a>TWE-2</h4><p>主题模型将单词根据它们的语义意思分成许多主题组。换句话说，一个词在不同的主题组对应着不同意思。因为大多数的词有多个意思，同时这些意思应该有在语义空间中有能够区别的向量。</p><p>因此，在 TWE-2 中我们考虑每一个词-主题对 $\langle w,z\rangle​$ 为伪词，并学习唯一的向量 $\mathbf{w}^z​$。 TWE-2 的目标是最大平均对数概率</p><script type="math/tex; mode=display">\mathcal{L}(D)=\frac{1}{M}\sum_{i=1}^M\sum_{-k\leq c\leq k,c\neq0}logPr(\langle w_{i+c},z_{i+c}\rangle|\langle w_i,z_i\rangle),\tag{8}</script><p>其中 $Pr(\langle w_c, z_c\rangle|\langle w_i, z_i\rangle)$ 同样是 softmax 函数</p><script type="math/tex; mode=display">Pr(\langle w_c, z_c\rangle|\langle w_i, z_i\rangle)=\frac{exp(\mathbf{w}_c^{z_c}\cdot\mathbf{w}_i^{z_i})}{\sum_{\langle w_c,z_c\rangle\in\langle W,T\rangle}exp(\mathbf{w}_c^{z_c}\cdot\mathbf{w}_i^{z_i})}.\tag{9}</script><p>通过这个方式，TWE-2 自然地将一个单词 $w​$ 分成 $T​$ 部分，其中 $\mathbf{w} = \sum_{z\in T}Pr(z|w)\mathbf{w}_z​$ 其中 $Pr(z|w)​$ 能够从 LDA 中得到。</p><p>在 TWE-2 中，我们通过 Eq.(4) 获得了一个单词的上下文词嵌入，可以用来进一步计算上下文单词相似度。对于文档嵌入向量，我们用 TWE-1 中相同的思路，通过 $\mathbf{d}=\sum_{\langle w, z\rangle\in d}Pr(\langle w, z\rangle|d)\mathbf{w}^z$ 获得文档嵌入向量</p><h4 id="TWE-3"><a href="#TWE-3" class="headerlink" title="TWE-3"></a>TWE-3</h4><p>自从 TWE-2 将每一个词分成不同的主题，相比于 Skip-Gram学习词嵌入任然有稀疏的问题。为了解决这个问题，我们提出了 TWE-3 模型来平衡单词鉴别力和稀疏。</p><p>和 TWE-1 类似，TWE-3 拥有词和主题的向量。对于每一个词-主题对 $\langle w,z\rangle$，TWE-3 将向量 $\mathbf{w}$ 和向量 $\mathbf{z}$ 连接建立向量 $\mathbf{w}^z$，其中$\lvert \mathbf{w}^z\rvert=\lvert \mathbf{w}\rvert + \lvert \mathbf{z}\rvert$。词向量长度 $\lvert \mathbf{w}\rvert$ 不需要和主题向量长度 $\vert \mathbf{z}\rvert$ 相等。</p><p>TWE-3 的目标和 TWE-2 的目标是相同的，是 Eq.(8)。 概率 $Pr(\langle w_c, z_c\rangle|\langle w_i, c_i\rangle)$ 与 Eq.(9) 中的相同。在 TWE-3 中，每个词嵌入 $\mathbf{w}$ 和主题嵌入 $\mathbf{z}$ 的参数在词-主题对 $\langle w, z\rangle$ 上共享；然而在 TWE-2中每个词-主题对 $\langle w, z\rangle$ 都有它们自己的参数。因此在 TWE-2 中词-主题对 $\langle w, z\rangle$ 和 $\langle w, z’\rangle$ 有着不同的参数，但是在 TWE-3 中它们共享了同一个单词嵌入 $\mathbf{w}$。</p><p>在 TWE-3 中，构建情景词嵌入和文档嵌入和 TWE-1中的一样。</p><h4 id="Optimization-and-Parameter-Estimation"><a href="#Optimization-and-Parameter-Estimation" class="headerlink" title="Optimization and Parameter Estimation"></a>Optimization and Parameter Estimation</h4><p>在学习 TWE 模型的过程中我们使用与 Skip-Gram (Mikolov et al. 2013) 中一样的优化方法。即使用随机梯度下降 (SGD) 进行优化，使用反向传播算法计算梯度。</p><p>在学习 TWE 模型中初始化十分重要。在 TWE-1 中，我们先从 Skip-Gram 中学习单词嵌入，然后我们用分配给这个主题的所有单词的平均值来初始化每个主题向量，同时在考试单词嵌入不变的情况下学习主题嵌入。在 TWE-2 中，我们用 Skip-Gram 中的相关的词向量初始化每个词-主题对，然后学习 TWE 模型。在 TWE-3 中，我们使用 Skip-Gram 初始化词向量，使用 TWE-1 中的主题向量，然后学习 TWE 模型。</p><h4 id="Complexity-Analysis"><a href="#Complexity-Analysis" class="headerlink" title="Complexity Analysis"></a>Complexity Analysis</h4><p>表 1 展示了多个模型的复杂度，包括模型参数的数量、计算复杂度。在这个表中，主题的数量是 $T$，词汇表的大小是 $W$， 窗口大小是 $C$， 语料库的大小是 $M$，词向量和主题向量的长度分别是 $K_W$ 和 $K_T$。在 Skip-Gram，TWE-1 和 TWE-2 中 $K_W=K_T=K$。我们没有计算主题模型的参数计算到 TWE 中，这些参数大约是 $O(WT)​$。</p><center>Table 1: Model complexities.</center><div class="table-container"><table><thead><tr><th style="text-align:center">模型</th><th style="text-align:center">模型参数</th><th style="text-align:center">计算复杂度</th></tr></thead><tbody><tr><td style="text-align:center">Skip-Gram</td><td style="text-align:center">$WK$</td><td style="text-align:center">$CM(1+logW)$</td></tr><tr><td style="text-align:center">TWE-1</td><td style="text-align:center">$(W+T)K$</td><td style="text-align:center">$IM+2CM(1+logW)$</td></tr><tr><td style="text-align:center">TWE-2</td><td style="text-align:center">$WTK$</td><td style="text-align:center">$IM+CM(1+logWT)$</td></tr><tr><td style="text-align:center">TWE-3</td><td style="text-align:center">$WK_W+TK_T$</td><td style="text-align:center">$IM+CM(1+logWT)$</td></tr></tbody></table></div><p>在计算复杂度上，$O(ID)$ 表示学习主题模型是的计算复杂度，其中 $I$ 表示迭代次数，第二项表示利用分层 softmax 学习主题嵌入的计算复杂度。</p><p>从复杂度分析我们可以知道，相比于 Skip-Gram，TWE 模型需要更多的参数来记录来为词嵌入提升鉴别力，但是计算复杂度缺没有上升，特别是在主题模型有更多的快速算法可用时。</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>在这一部分，我们通过两个任务来评估相关的模型，包括上下文单词相似度和文本分类，</p><h4 id="Contextual-Word-Similarity"><a href="#Contextual-Word-Similarity" class="headerlink" title="Contextual Word Similarity"></a>Contextual Word Similarity</h4><p>评估词嵌入的传统任务是忽视上下文信息计算词相似度，使用标准数据集如WordSim353（Finkelstein et al. 2001）。但是大多数单词的语义高度依赖语境。因此我们使用上下文词相似度来考察主题词嵌入的有效性。</p><h5 id="Datasets-and-Experiment-Setting"><a href="#Datasets-and-Experiment-Setting" class="headerlink" title="Datasets and Experiment Setting"></a>Datasets and Experiment Setting</h5><p>我们使用了（Huang et al. 2012）发布的数据集 SCWS 来评估。在 SCWS 中有 2003 对词语和包含这些词语的句子；还有人类给出的基于上下文的词语相似分数。我们对比了模型和人类判断分数的斯皮尔曼相关系数。对于 TWE 模型来说，两个词之间的相似度使用 Eq.(5) 计算。</p><p>我们选择了最大的在线知识库——维基百科来学习主题词嵌入，我们采用了2010年4月的版本（Huang et al. 2012）。当使用 LDA 学习主题时，设置 $T=400$ 和 $I = 50$。在学习 Skip-Gram 和 TWE 模型时，我们这是了窗口大小为 5，词嵌入和主题嵌入的维度都是 $K = 400$。对于 TWE-1 和 TWE-3 来说，我们通过连接相应的词嵌入和主题嵌入，获得了主题词嵌入；对于 TWE-2 来说，主题词嵌入是现成的。</p><p>在实验中，我们将我们的模型和下面几种基准模型进行了对比</p><ul><li><strong>C&amp;W 模型</strong>（Collobert and Weston 2008）：使用（Collobert and Weston 2008）提出的词嵌入，忽略上下文信息。</li><li><strong>TFIDF</strong> 包括 pruned TF IDF 和 pruned TF IDF-M（Reiginger and Mooney 2010）：使用 TFIDF 加权的窗口为 10 的上下文来表示一个单词。精简的 TF IDF 通过减少分数低的上下文来提高性能。 </li><li><strong>Huang的模型</strong>（Huang et al. 2012）：通过移除词向量中除了 top 200 单词意外的单词来精简。</li><li><strong>Tian的模型</strong>（Tian et al. 2014）：提出了学习多原型词嵌入的概率模型，相比于 Huang 的模型，其性能相差不大，但是效率更高。</li><li><strong>LDA</strong>：我们有两种计算上下文相似度的方法。<strong>LDA-S</strong> 忽略上下文，使用每一个单词的主题分布来 $Pr(z|w) \propto Pr(w|z) Pr(z)$ 表示单词，其中的 $Pr(w|z)$ 和 $Pr(z)$ 可以从 LDA 模型中获得。<strong>LDA-C</strong> 模型将单词 $w$ 的上下文句子 $c$ 视为一个文档，使用上下文主题分布 $Pr(z|w,c)$ 来表示单词，并且计算单词相似度。很明显的 LDA-S 忽视上下文，而 LDA-C 是上下文敏感的。 </li><li><strong>Skip-Gram</strong>：忽略上下文，使用词向量来计算相似度，维度 $K = 400$。</li></ul><p>在这些基准模型中，C&amp;W、TFIDF、Pruned TFIDF、LDA-S、LDA-C 和 Skip-Gram 是单原型模型，而 TFIDF-M、Huang 和 Tian 是多原型模型。 </p><h5 id="Evaluation-Result"><a href="#Evaluation-Result" class="headerlink" title="Evaluation Result"></a>Evaluation Result</h5><p>在表2中，我们展示了在 SCWS 数据集上的各个模型的评估结果。对于所有的多原型模型和 TWE 模型我们使用了 AvgSimC 和 MaxSimC 来评估结果。结果如下表</p><p>Table 2： SCWS 数据集上下文相似性的斯皮尔曼相关系数 $\rho \times 100​$。</p><div class="table-container"><table><thead><tr><th style="text-align:center">Model</th><th style="text-align:center">$\rho \times 100$</th></tr></thead><tbody><tr><td style="text-align:center">$\text{C&amp;W}$</td><td style="text-align:center">$\text{57.0}$</td></tr><tr><td style="text-align:center">$\text{TFIDF}$</td><td style="text-align:center">$\text{26.3}$</td></tr><tr><td style="text-align:center">$\text{Pruned TFIDF}$</td><td style="text-align:center">$\text{62.5}$</td></tr><tr><td style="text-align:center">$\text{LDA-S}$</td><td style="text-align:center">$\text{56.9}$</td></tr><tr><td style="text-align:center">$\text{LDA-C}$</td><td style="text-align:center">$\text{50.4}$</td></tr><tr><td style="text-align:center">$\text{Skip-Gram}$</td><td style="text-align:center">$\text{65.7}$</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">AvgSimC</th><th style="text-align:center">MaxSimC</th></tr></thead><tbody><tr><td style="text-align:center">$\text{Pruned TFIDF-M}$</td><td style="text-align:center">$\text{60.5}$</td><td style="text-align:center">$\text{60.4}$</td></tr><tr><td style="text-align:center">$\text{Tian}$</td><td style="text-align:center">$\text{65.4}$</td><td style="text-align:center">$\text{63.6}$</td></tr><tr><td style="text-align:center">$\text{Huang}$</td><td style="text-align:center">$\text{65.3}$</td><td style="text-align:center">$\text{58.6}$</td></tr><tr><td style="text-align:center">$\text{TWE-1}$</td><td style="text-align:center">$\textbf{68.1}$</td><td style="text-align:center">$\textbf{67.3}$</td></tr><tr><td style="text-align:center">$\text{TWE-2}$</td><td style="text-align:center">$\text{67.9}$</td><td style="text-align:center">$\text{63.6}$</td></tr><tr><td style="text-align:center">$\text{TWE-3}$</td><td style="text-align:center">$\text{67.1}$</td><td style="text-align:center">$\text{65.5}$</td></tr></tbody></table></div><p>TWE 模型表现优于所有其他模型，其中 TWE-1 模型的表现最好。这表明 TWE 模型能够有效的利用潜在话题模型进行话题词嵌入。</p><p>对于多原型模型，TWE 模型在计算 MaxSimC 上比其他模型都要好。通过选择最有可能的主题词嵌入，使得TWE模型能够更加准确地表达上下文的语义。这表明 TWE 模型相比于其他多原型模型具有以下两个优点：</p><ol><li>在大多数多原型模型，每一个单词通常只有有限个原型（在 Huang 的模型中是 10 个）。TWE 模型通过利用潜在的主题模型，在主题空间（通常是数百个维度）中区分单词语义。</li><li>大多数多原型模型分别对每个单词建立多个原型，忽略了单词和它的上下文之间的联系。采用主题模型之后，我们能够通过一起考虑词和上下文来区分单词语义。使用主题模型还提供更多原则方式来选择出基于具体的上下文的最合适的主题词嵌入。</li></ol><p>TWE 三种模型分别反应了它们各自的特点：</p><ol><li>使用 AvgSimC 时，TWE-2 和 TWE-1 相当，当使用 MaxSimC 时 TWE-2 的效果比较差。我们猜测原因是当使用 MaxSimC 时，TWE-2 学习不同的词-主题嵌入导致了稀疏性问题。</li><li>TWE-2 的性能在 TWE-1 和 TWE-3 之间。原因是在 TWE-3 的学习过程中，主题嵌入会影响词嵌入，单只主题词嵌入在下一个主题下的区分性比较小。</li></ol><h4 id="Text-Classification"><a href="#Text-Classification" class="headerlink" title="Text Classification"></a>Text Classification</h4><p>我们使用文本分类进行文本建模从而考察 TWE 模型的效果。</p><h5 id="Datasets-and-Experiment-Setting-1"><a href="#Datasets-and-Experiment-Setting-1" class="headerlink" title="Datasets and Experiment Setting"></a>Datasets and Experiment Setting</h5><p>多类文本分类在 NLP 和 IR 中是一个很好研究的问题。在本文中，我们在数据集 20NewsGroup 上进行实验。20NewsGroup 包括了 20 个不同的新闻组一共 20,000 文档。我们记录了宏观平均 precision、recall 和 F-measure 进行比较。</p><p>对于 TWE 模型，我们在训练集使用 LDA 来学习主题模型，设置主题的数量为 $T = 80$。我们进一步地在训练集学习了主题词嵌入，然后为训练集和测试集生成文档嵌入。接下来，我们将文档嵌入向量当做文档特征，使用 Liblinear（Fan et al. 2008）训练线性分类器。我们将词嵌入和主题嵌入的维度设定为 $K = 400$。</p><p>我们考虑了一下几种基准：词袋（BOW）模型，LDA，Skip-Gram 和 Paragraph Vector（PV）模型（Le and  Mikolov 2014）。</p><ul><li>其中 BOW 模型将每个文档表示为一个词袋，并使用 TFIDF 值作为权重，对于 TFIDF，我们选择前 50,000 分值的词作为特征。</li><li>LDA 将每个文档表示为其潜在的主题分布，我们使用 TWE 中的 LDA 模型。</li><li>在 Skip-Gram 模型中农，我们通过文档内所有的词嵌入向量的平均值来构建文档的嵌入向量，维度是 $K = 400$。</li><li>Paragraph Vector 模型是最近提出的文档嵌入模型，它包括分布式内存模型（PV-DM）和分布式词袋模型（PV-DBOW）。PV模型实现了目前最好的情感分类功能（Le and Mikolov 2014），但是还没有公布源码，doc2vec 提供了在线接口。在实验中我们发现其性能在比我们差，因此只展示结果以作对比。</li></ul><h5 id="Evaluation-Result-1"><a href="#Evaluation-Result-1" class="headerlink" title="Evaluation Result"></a>Evaluation Result</h5><p>表 3 展示在数据集 20NewsGroup 上进行文本分类的结果评估。我们可以发现 TWE-1 表现最好，特别是在主题模型和嵌入模型上。这说明了我们的模型能够更精准地捕捉文档的语义信息。而且相比于 BOW 模型，TWE 模型在这种情况下能够将文档特征降低 99.2%。</p><center>Table 3: 多类文本分类的评估结果</center><div class="table-container"><table><thead><tr><th style="text-align:center">Model</th><th style="text-align:center">Accuracy</th><th style="text-align:center">Precision</th><th style="text-align:center">Recall</th><th style="text-align:center">F-measure</th></tr></thead><tbody><tr><td style="text-align:center">$\text{BOW}$</td><td style="text-align:center">$\text{79.7}$</td><td style="text-align:center">$\text{79.5}$</td><td style="text-align:center">$\text{79.0}$</td><td style="text-align:center">$\text{79.0}$</td></tr><tr><td style="text-align:center">$\text{LDA}$</td><td style="text-align:center">$\text{72.2}$</td><td style="text-align:center">$\text{70.8}$</td><td style="text-align:center">$\text{70.7}$</td><td style="text-align:center">$\text{70.0}$</td></tr><tr><td style="text-align:center">$\text{Skip-Gram}$</td><td style="text-align:center">$\text{75.4}$</td><td style="text-align:center">$\text{75.1}$</td><td style="text-align:center">$\text{74.3}$</td><td style="text-align:center">$\text{74.2}$</td></tr><tr><td style="text-align:center">$\text{PV-DM}$</td><td style="text-align:center">$\text{72.4}$</td><td style="text-align:center">$\text{72.1}$</td><td style="text-align:center">$\text{71.5}$</td><td style="text-align:center">$\text{71.5}$</td></tr><tr><td style="text-align:center">$\text{PV-DBOW}$</td><td style="text-align:center">$\text{75.4}$</td><td style="text-align:center">$\text{74.9}$</td><td style="text-align:center">$\text{74.3}$</td><td style="text-align:center">$\text{74.3}$</td></tr><tr><td style="text-align:center">$\text{TWE-1}$</td><td style="text-align:center">$\textbf{81.5}$</td><td style="text-align:center">$\textbf{81.2}$</td><td style="text-align:center">$\textbf{80.6}$</td><td style="text-align:center">$\textbf{80.6}$</td></tr><tr><td style="text-align:center">$\text{TWE-2}$</td><td style="text-align:center">$\text{79.0}$</td><td style="text-align:center">$\text{78.6}$</td><td style="text-align:center">$\text{77.9}$</td><td style="text-align:center">$\text{77.9}$</td></tr><tr><td style="text-align:center">$\text{TWE-3}$</td><td style="text-align:center">$\text{77.4}$</td><td style="text-align:center">$\text{77.2}$</td><td style="text-align:center">$\text{76.2}$</td><td style="text-align:center">$\text{76.1}$</td></tr></tbody></table></div><p>在三种 TWE 模型中，最简单的 TWE-1 模型获得了最好的性能。TWE-1 中采用独立性假设可能是表现更好的原因。另外，20NewsGroup 数据集较小，我们猜测提供更多的数据集时 TWE-2 和 TWE-3 性能表现会提升。未来我们将进行更多实验。</p><h4 id="Examples-of-Topical-Word-Embeddings"><a href="#Examples-of-Topical-Word-Embeddings" class="headerlink" title="Examples of Topical Word Embeddings"></a>Examples of Topical Word Embeddings</h4><p>为了展示 TWE 模型的特点。我们选择了一些单词以及使用 TWE 模型获得不同主题下最相似的词。我们用了 Skip-Gram 来作为对比。</p><p>在表格4中，我们展示了$bank$、$left$、$apple$ 三个词的最相似的词。对于词 $w$，我们第一行是 Skip-Gram模型获得的相似词，第二行和第三行是使用 TWE-2 获得的不同主题下的相似词，为 w#1 和 w#2。</p><center>Table 4: TWE-2 和 Skip-Gram 中最相近的词</center><div class="table-container"><table><thead><tr><th style="text-align:center">Words</th><th style="text-align:center">Similar Word</th></tr></thead><tbody><tr><td style="text-align:center">bank</td><td style="text-align:center">citibank, investment, river</td></tr><tr><td style="text-align:center">bank#1</td><td style="text-align:center">insurance, stock, investor</td></tr><tr><td style="text-align:center">bank#2</td><td style="text-align:center">river, edge, coast</td></tr><tr><td style="text-align:center">left</td><td style="text-align:center">right, leave, quit</td></tr><tr><td style="text-align:center">left#1</td><td style="text-align:center">moved, arrived, leave</td></tr><tr><td style="text-align:center">left#2</td><td style="text-align:center">right, bottom, hand</td></tr><tr><td style="text-align:center">apple</td><td style="text-align:center">macintosh, ios, juice</td></tr><tr><td style="text-align:center">apple#1</td><td style="text-align:center">peach, juice, strawberry</td></tr><tr><td style="text-align:center">apple#2</td><td style="text-align:center">mac, ipod, android</td></tr></tbody></table></div><p>从表4中我们可以看到，Skip-Gram 中的相似词包含了多个意思。这表明 Skip-Gram 将多个语义组合成唯一一个词嵌入向量。相反的，TWE 模型中我们可以成功地区分多个主题下的单词语义。</p><h3 id="Related-Word"><a href="#Related-Word" class="headerlink" title="Related Word"></a>Related Word</h3><p>IR 和 NLP 任务的关键取决于文本表示，而文本表示的基础是词表示。one-hot word representation 方法将每个单词表示为一个非零向量，其被广泛用为词袋（BOW）文档模型的基础（Manning, Raghavan and Schutze 2008）。然而存在着许多挑战，其中最关键的是能否考虑单词之间的语义和句法关系。</p><ul><li>词嵌入（Rumelhart, Hintont, and Williams 1986）</li><li>词嵌入用于语言模型（Bengio et al. 2006; Mnih and Hinton 2008）</li><li>词嵌入用于命名实体识别（Turian, Ratinov, and Bengio 2010）</li><li>词嵌入用于消除歧义（Collobert et al. 2011）</li><li>词嵌入用于解析（Socher et al. 2011）</li></ul><p>词嵌入可以将单词的语法和语义信息编码成连续向量，相似的单词在向量空间中的位置相近。</p><p>优于计算度复杂，以前的词嵌入都是耗时的。最近（Mikolov et al. 2013）提出了两种有限的模型，Skip-Gram 和连续词袋模型（CBOW）来从大规模文本语料库中学习词嵌入。CBOW的训练目标是结合上下文的词嵌入来预测目标词，而 Skip-Gram 则是用每个目标词的嵌入来预测其上下文词。</p><p>以前的词嵌入模型针对每个词只有唯一的向量表示，无法区分多种语义，因此研究人员提出了多原型模型：</p><ul><li>多原型向量空间模型（Reisinger and Mooney 2010），将每一个目标词的上下文分组，为每个组构建上下文向量。</li><li>Huang的模型（Huang et al. 2012）：将上下文聚合，每个聚类产生一个唯一原型嵌入。</li><li>概率模型（Tian et al. 2014）</li><li>双语模型（Guo et al. 2014）</li><li>非参数模型（Neelakantan et al. 2014）</li></ul><p>这些方法大多数都是为每个词构建多原型词嵌入。相对的，TEW模型使用考虑全部单词和上下文来确定潜在的主题来确定词意思。此外多原型模型能够被 TWE 模型收录。</p><h3 id="Conclusion-and-Future-Work"><a href="#Conclusion-and-Future-Work" class="headerlink" title="Conclusion and Future Work"></a>Conclusion and Future Work</h3><p>本文中我们提出了三种主题词嵌入模型，用来进行上下文的词嵌入和文档嵌入。我们在两种任务上评估了我们的模型：基于语境的词相似度和文本分类。实验结果表明我们的模型，尤其是 TWE-1 在词相似度任务中表现最好，在文本分类中也具有不错的表现。</p><p>我们从以下几个方面考虑未来的研究方向：</p><ul><li>在 LDA 中，主题数量需要预定确定。交叉验证可以用来找到合适的主题数，但不适合于大规模数据。我们将探讨非参数主题模型（Teh et al. 2006）的主题词嵌入。</li><li>还有很多知识库可用，如 WordNet（Miller 1995），含有丰富的同音异义和多义词的语言知识。我们可能会套索将这些先验知识整合到主题词嵌入中的技巧。</li><li>文档中通常包含其他信息，如分类标签、超链接和时间戳。我们可以将这些信息用于学习更具代表性的主题模型（Mcauliffe and Blei 2008; Zhu, Ahmed, and Xing 2009; Lacoste-Julien, Sha, and Jordan 2009），增强主题词嵌入。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Topical-Word-Embeddings&quot;&gt;&lt;a href=&quot;#Topical-Word-Embeddings&quot; class=&quot;headerlink&quot; title=&quot;Topical Word Embeddings&quot;&gt;&lt;/a&gt;Topical Word Embeddings&lt;/h2&gt;&lt;p&gt;Yang Liu, Zhiyuan Liu, Tat-Seng Chua, µ Sun&lt;/p&gt;
&lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;p&gt;大多数的词嵌入模型通常使用单个向量来表示每一个单词，这使得这些模型在同音异义和一词多义上无法区分。为了增强辨别力，我们采用潜在主题模型（latent topic model）为语料库里的每一个词分配主题，并且基于词和主题来学习&lt;strong&gt;主题词嵌入（TWE）&lt;/strong&gt;。这样能够灵活地获得语境词嵌入（contextual word embedding），来衡量语境中词的相似性。我们还可以构建文档向量表示（document representation），相比广泛使用的文档模型（如潜主题模型）更具表现力。在实验中，我们评估了 TWE 模型的两个任务：情景词相似性和文本分类。实验结果表明，我们的模型比经典的词嵌模型（包括基于语境相似度多种原型版本）表现好，同时在文本分类上超过了潜在主题模型和其他代表性的文档模型。本文的源代码能够在&lt;a href=&quot;https://github.com/largelymfs/topical_word_embeddings&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/largelymfs/topical_word_embeddings&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="论文翻译" scheme="http://blog.dhquan.cn/categories/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    
    
      <category term="翻译" scheme="http://blog.dhquan.cn/tags/%E7%BF%BB%E8%AF%91/"/>
    
      <category term="论文" scheme="http://blog.dhquan.cn/tags/%E8%AE%BA%E6%96%87/"/>
    
      <category term="Word Embeddings" scheme="http://blog.dhquan.cn/tags/Word-Embeddings/"/>
    
  </entry>
  
  <entry>
    <title>Docker 容器与macOS宿主时间同步</title>
    <link href="http://blog.dhquan.cn/2019/02/09/Docker%20%E5%AE%B9%E5%99%A8%E4%B8%8E%E5%AE%BF%E4%B8%BB%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5/"/>
    <id>http://blog.dhquan.cn/2019/02/09/Docker 容器与宿主时间同步/</id>
    <published>2019-02-09T17:41:11.000Z</published>
    <updated>2019-02-09T10:08:37.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Docker-容器与macOS宿主时间同步"><a href="#Docker-容器与macOS宿主时间同步" class="headerlink" title="Docker 容器与macOS宿主时间同步"></a>Docker 容器与macOS宿主时间同步</h2><p>通过 Docker 搭建了自己的个人博客之后，在同步到 Github 上的时候发现 commit 的时间有问题。进入到容器中看了下，发现容器的时间是 UTC+0， 而我的电脑是 UTC+8</p><h3 id="复制主机的-localtime"><a href="#复制主机的-localtime" class="headerlink" title="复制主机的 localtime"></a>复制主机的 localtime</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker cp /etc/localtime hexo-server:/etc/</span><br><span class="line"><span class="meta">$</span> docker cp /etc/localtime hexo-deploy:/etc/</span><br></pre></td></tr></table></figure><p>但是在 macOS 会出现报错</p><p>查了下发现 <code>/etc/localtime</code> 是一个链接文件，真正的 <code>localtime</code> 是 <code>/var/db/timezone/zoneinfo/Asia/Shanghai</code> 这个文件。</p><p>把这个真正的 <code>localtime</code> 文件复制到容器就可以了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> cat /var/db/timezone/zoneinfo/Asia/Shanghai &gt; ~/localtime</span><br><span class="line"><span class="meta">$</span> docker cp ~/localtime hexo-server:/etc/</span><br><span class="line"><span class="meta">$</span> docker cp ~/localtime hexo-deploy:/etc/</span><br></pre></td></tr></table></figure><h3 id="创建-Dockerfile-的时候将-localtime-文件替换"><a href="#创建-Dockerfile-的时候将-localtime-文件替换" class="headerlink" title="创建 Dockerfile 的时候将 localtime 文件替换"></a>创建 Dockerfile 的时候将 localtime 文件替换</h3><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">COPY</span> localtime /etc/localtime</span><br></pre></td></tr></table></figure><p>将上述语句写入 Dockerfile 中。重新 docker build 生成新的 Image 就可以了。</p><p>因为我已经通过第一个方法解决了时间问题，就没有重新生成新的 Image 了。</p><h3 id="挂载主机的-localtime-到容器中"><a href="#挂载主机的-localtime-到容器中" class="headerlink" title="挂载主机的 localtime 到容器中"></a>挂载主机的 localtime 到容器中</h3><p>已我的 Hexo-server 为例，在第 6 行添加。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -p 4000:80 --name hexo-server -d \</span><br><span class="line">-v &#123;你的博客文件夹路径&#125;/.ssh:/root/.ssh \</span><br><span class="line">-v &#123;你的博客文件夹路径&#125;/source:/Hexo/source \</span><br><span class="line">-v &#123;你的博客文件夹路径&#125;/themes:/Hexo/themes \</span><br><span class="line">-v &#123;你的博客文件夹路径&#125;/_config.yml:/Hexo/_config.yml \</span><br><span class="line">-v ~/localtime:/etc/localtime</span><br><span class="line">xxx/hexo &#123;你的 github username&#125; &#123;你的 github email&#125; server</span><br></pre></td></tr></table></figure><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.jianshu.com/p/ce5408b33972" target="_blank" rel="noopener">Docker 容器与主机时间同步</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Docker-容器与macOS宿主时间同步&quot;&gt;&lt;a href=&quot;#Docker-容器与macOS宿主时间同步&quot; class=&quot;headerlink&quot; title=&quot;Docker 容器与macOS宿主时间同步&quot;&gt;&lt;/a&gt;Docker 容器与macOS宿主时间同步&lt;/
      
    
    </summary>
    
      <category term="建站" scheme="http://blog.dhquan.cn/categories/%E5%BB%BA%E7%AB%99/"/>
    
    
      <category term="建站" scheme="http://blog.dhquan.cn/tags/%E5%BB%BA%E7%AB%99/"/>
    
      <category term="Docker" scheme="http://blog.dhquan.cn/tags/Docker/"/>
    
      <category term="macOS" scheme="http://blog.dhquan.cn/tags/macOS/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 中开启及修复渲染 MathJax 问题</title>
    <link href="http://blog.dhquan.cn/2019/02/04/Hexo-%E4%B8%AD%E5%BC%80%E5%90%AF%E5%8F%8A%E4%BF%AE%E5%A4%8D%E6%B8%B2%E6%9F%93-MathJax-%E9%97%AE%E9%A2%98/"/>
    <id>http://blog.dhquan.cn/2019/02/04/Hexo-中开启及修复渲染-MathJax-问题/</id>
    <published>2019-02-04T18:59:00.000Z</published>
    <updated>2019-02-04T11:31:28.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Hexo-中开启及修复渲染-MathJax-问题"><a href="#Hexo-中开启及修复渲染-MathJax-问题" class="headerlink" title="Hexo 中开启及修复渲染 MathJax 问题"></a>Hexo 中开启及修复渲染 MathJax 问题</h2><p>因为写博客的时候难免会用到数学公式，所以准备配置下 Hexo 中的 MathJax 。但是默认的情况下渲染数学公式会出现各种各样的问题</p><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>Hexo 默认使用 hexo-renderer-marked 引擎渲染网页，该引擎会把一些特殊的 markdown 符号转换为相应的 html 标签，比如在 markdown 语法中，下划线<code>_</code>代表斜体，会被渲染引擎处理为<code>&lt;em&gt;</code>标签。</p><p>因为类 Latex 格式书写的数学公式下划线<code>_</code>表示下标，有特殊的含义，如果被强制转换为<code>&lt;em&gt;</code>标签，那么 MathJax 引擎在渲染数学公式的时候就会出错。</p><p>类似的语义冲突的符号还包括<code>*</code>, <code>{</code>, <code>}</code>, <code>\\</code>等。</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><h4 id="替换-Hexo-的-Markdown-渲染引擎，使用-hexo-renderer-kramed-引擎"><a href="#替换-Hexo-的-Markdown-渲染引擎，使用-hexo-renderer-kramed-引擎" class="headerlink" title="替换 Hexo 的 Markdown 渲染引擎，使用 hexo-renderer-kramed 引擎"></a>替换 Hexo 的 Markdown 渲染引擎，使用 hexo-renderer-kramed 引擎</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> npm uninstall hexo-renderer-marked --save</span><br><span class="line"><span class="meta">$</span> npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="修改引擎中的-inline-js-文件"><a href="#修改引擎中的-inline-js-文件" class="headerlink" title="修改引擎中的 inline.js 文件"></a>修改引擎中的 inline.js 文件</h4><p>修改 { 博客根目录下 } /node_modules/kramed/lib/rules/inline.js 中的 escape 和 em 变量。</p><p>修改第 11 行的 escape 变量</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span></span><br><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([`*\[\]()#$+\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure><p>修改第 20 行的 em 变量</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span></span><br><span class="line">em: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure><h4 id="开启-Next-主题中-MathJax-开关"><a href="#开启-Next-主题中-MathJax-开关" class="headerlink" title="开启 Next 主题中 MathJax 开关"></a>开启 Next 主题中 MathJax 开关</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">math:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Default(true) will load mathjax/katex script on demand</span></span><br><span class="line">  <span class="comment"># That is it only render those page who has `mathjax: true` in Front Matter.</span></span><br><span class="line">  <span class="comment"># If you set it to false, it will load mathjax/katex srcipt EVERY PAGE.</span></span><br><span class="line"><span class="attr">  per_page:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  engine:</span> <span class="string">mathjax</span></span><br><span class="line">  <span class="comment">#engine: katex</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># hexo-rendering-pandoc (or hexo-renderer-kramed) needed to full MathJax support.</span></span><br><span class="line"><span class="attr">  mathjax:</span></span><br><span class="line">    <span class="comment"># Use 2.7.1 as default, jsdelivr as default CDN, works everywhere even in China</span></span><br><span class="line"><span class="attr">    cdn:</span> <span class="string">//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML</span></span><br><span class="line">    <span class="comment"># For direct link to MathJax.js with CloudFlare CDN (cdnjs.cloudflare.com)</span></span><br><span class="line">    <span class="comment">#cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># See: https://mhchem.github.io/MathJax-mhchem/</span></span><br><span class="line">    <span class="comment">#mhchem: //cdn.jsdelivr.net/npm/mathjax-mhchem@3</span></span><br><span class="line">    <span class="comment">#mhchem: //cdnjs.cloudflare.com/ajax/libs/mathjax-mhchem/3.3.0</span></span><br></pre></td></tr></table></figure><h4 id="在文章的-Front-matter-里打开-mathjax-开关"><a href="#在文章的-Front-matter-里打开-mathjax-开关" class="headerlink" title="在文章的 Front-matter 里打开 mathjax 开关"></a>在文章的 Front-matter 里打开 mathjax 开关</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">xxx</span></span><br><span class="line"><span class="attr">date:</span> <span class="string">xxxx-xx-xx</span> <span class="attr">xx:xx:xx</span></span><br><span class="line"><span class="attr">tags:</span></span><br><span class="line"><span class="attr">mathjax:</span> <span class="literal">true</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure><h3 id="MathJax-测试"><a href="#MathJax-测试" class="headerlink" title="MathJax 测试"></a>MathJax 测试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;eqnarray&#125;</span><br><span class="line">\nabla\cdot\vec&#123;E&#125; &amp;=&amp; \frac&#123;\rho&#125;&#123;\epsilon_0&#125; \\</span><br><span class="line">\nabla\cdot\vec&#123;B&#125; &amp;=&amp; 0 \\</span><br><span class="line">\nabla\times\vec&#123;E&#125; &amp;=&amp; -\frac&#123;\partial B&#125;&#123;\partial t&#125; \\</span><br><span class="line">\nabla\times\vec&#123;B&#125; &amp;=&amp; \mu_0\left(\vec&#123;J&#125;+\epsilon_0\frac&#123;\partial E&#125;&#123;\partial t&#125; \right)</span><br><span class="line">\end&#123;eqnarray&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{eqnarray}\nabla\cdot\vec{E} &=& \frac{\rho}{\epsilon_0} \\\nabla\cdot\vec{B} &=& 0 \\\nabla\times\vec{E} &=& -\frac{\partial B}{\partial t} \\\nabla\times\vec{B} &=& \mu_0\left(\vec{J}+\epsilon_0\frac{\partial E}{\partial t} \right)\end{eqnarray}</script><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol><li><a href="https://blog.csdn.net/wgshun616/article/details/81019687" target="_blank" rel="noopener">Hexo 的 Next 主题中渲染 MathJax 数学公式</a></li><li><a href="https://zhuanlan.zhihu.com/p/33857596" target="_blank" rel="noopener">Hexo博客使用MathJax公式并解决Markdown渲染冲突问题</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Hexo-中开启及修复渲染-MathJax-问题&quot;&gt;&lt;a href=&quot;#Hexo-中开启及修复渲染-MathJax-问题&quot; class=&quot;headerlink&quot; title=&quot;Hexo 中开启及修复渲染 MathJax 问题&quot;&gt;&lt;/a&gt;Hexo 中开启及修复渲染 MathJax 问题&lt;/h2&gt;&lt;p&gt;因为写博客的时候难免会用到数学公式，所以准备配置下 Hexo 中的 MathJax 。但是默认的情况下渲染数学公式会出现各种各样的问题&lt;/p&gt;
&lt;h3 id=&quot;原因&quot;&gt;&lt;a href=&quot;#原因&quot; class=&quot;headerlink&quot; title=&quot;原因&quot;&gt;&lt;/a&gt;原因&lt;/h3&gt;&lt;p&gt;Hexo 默认使用 hexo-renderer-marked 引擎渲染网页，该引擎会把一些特殊的 markdown 符号转换为相应的 html 标签，比如在 markdown 语法中，下划线&lt;code&gt;_&lt;/code&gt;代表斜体，会被渲染引擎处理为&lt;code&gt;&amp;lt;em&amp;gt;&lt;/code&gt;标签。&lt;/p&gt;
&lt;p&gt;因为类 Latex 格式书写的数学公式下划线&lt;code&gt;_&lt;/code&gt;表示下标，有特殊的含义，如果被强制转换为&lt;code&gt;&amp;lt;em&amp;gt;&lt;/code&gt;标签，那么 MathJax 引擎在渲染数学公式的时候就会出错。&lt;/p&gt;
&lt;p&gt;类似的语义冲突的符号还包括&lt;code&gt;*&lt;/code&gt;, &lt;code&gt;{&lt;/code&gt;, &lt;code&gt;}&lt;/code&gt;, &lt;code&gt;\\&lt;/code&gt;等。&lt;/p&gt;
&lt;h3 id=&quot;解决方法&quot;&gt;&lt;a href=&quot;#解决方法&quot; class=&quot;headerlink&quot; title=&quot;解决方法&quot;&gt;&lt;/a&gt;解决方法&lt;/h3&gt;&lt;h4 id=&quot;替换-Hexo-的-Markdown-渲染引擎，使用-hexo-renderer-kramed-引擎&quot;&gt;&lt;a href=&quot;#替换-Hexo-的-Markdown-渲染引擎，使用-hexo-renderer-kramed-引擎&quot; class=&quot;headerlink&quot; title=&quot;替换 Hexo 的 Markdown 渲染引擎，使用 hexo-renderer-kramed 引擎&quot;&gt;&lt;/a&gt;替换 Hexo 的 Markdown 渲染引擎，使用 hexo-renderer-kramed 引擎&lt;/h4&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; npm uninstall hexo-renderer-marked --save&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; npm install hexo-renderer-kramed --save&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="建站" scheme="http://blog.dhquan.cn/categories/%E5%BB%BA%E7%AB%99/"/>
    
    
      <category term="建站" scheme="http://blog.dhquan.cn/tags/%E5%BB%BA%E7%AB%99/"/>
    
      <category term="Hexo" scheme="http://blog.dhquan.cn/tags/Hexo/"/>
    
      <category term="MathJax" scheme="http://blog.dhquan.cn/tags/MathJax/"/>
    
  </entry>
  
  <entry>
    <title>Hexo + Docker + Github 搭建个人博客</title>
    <link href="http://blog.dhquan.cn/2019/02/02/Hexo-+-Docker-+-Github-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>http://blog.dhquan.cn/2019/02/02/Hexo-+-Docker-+-Github-搭建个人博客/</id>
    <published>2019-02-02T21:58:00.000Z</published>
    <updated>2019-02-04T11:32:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录了在macOS上搭建Hexo博客的完整过程。之所以选择 Hexo + Docker + Github 搭建个人博客，是因为不想在 VPS 上搭建（写博客上传麻烦，之前搭过有被我弄坏了node环境），就准备在Docker上搭建。</p><h1 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h1><h2 id="Docker安装"><a href="#Docker安装" class="headerlink" title="Docker安装"></a>Docker安装</h2><h3 id="使用-Homebrew-安装"><a href="#使用-Homebrew-安装" class="headerlink" title="使用 Homebrew 安装"></a>使用 Homebrew 安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> brew cask install docker</span><br></pre></td></tr></table></figure><h3 id="更多的安装方法：官方文档"><a href="#更多的安装方法：官方文档" class="headerlink" title="更多的安装方法：官方文档"></a>更多的安装方法：<a href="https://yeasy.gitbooks.io/docker_practice/install/mac.html" target="_blank" rel="noopener">官方文档</a></h3><h2 id="Kitematic下载"><a href="#Kitematic下载" class="headerlink" title="Kitematic下载"></a>Kitematic下载</h2><p>Kitematic 是 Docker 官方的提供的可视化程序。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> brew cask install kitematic</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="Docker-Hexo-Image"><a href="#Docker-Hexo-Image" class="headerlink" title="Docker-Hexo Image"></a>Docker-Hexo Image</h2><p>能够在 Docker 上运行的 Hexo 镜像。</p><h3 id="1-网上公开的-Image"><a href="#1-网上公开的-Image" class="headerlink" title="1. 网上公开的 Image"></a>1. 网上公开的 Image</h3><p>这个镜像好像有一个 Bug， 所以我用的是第二种方法。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker pull iyannik0215/docker-hexo</span><br></pre></td></tr></table></figure><h3 id="2-自己通过-DockerFile-构建-Image"><a href="#2-自己通过-DockerFile-构建-Image" class="headerlink" title="2.自己通过 DockerFile 构建 Image"></a>2.自己通过 DockerFile 构建 Image</h3><h4 id="建立Dockerfile，具体Dockerfile的代码"><a href="#建立Dockerfile，具体Dockerfile的代码" class="headerlink" title="建立Dockerfile，具体Dockerfile的代码"></a>建立Dockerfile，具体Dockerfile的代码</h4><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> mhart/alpine-node:<span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">MAINTAINER</span> dhq , &lt;dhq@dhquan.cn&gt;</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span> \</span><br><span class="line">    apk --update --no-progress add git openssh</span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span> /Hexo</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span> \</span><br><span class="line">    npm install hexo-cli -g \</span><br><span class="line">    &amp;&amp; hexo init . \</span><br><span class="line">    &amp;&amp; npm install \</span><br><span class="line">    &amp;&amp; npm install hexo-generator-sitemap --save \</span><br><span class="line">    &amp;&amp; npm install hexo-generator-feed --save \</span><br><span class="line">    &amp;&amp; npm install hexo-deployer-git --save \</span><br><span class="line">    &amp;&amp; npm uninstall hexo-renderer-marked --save \</span><br><span class="line">    &amp;&amp; npm install hexo-renderer-kramed --save</span><br><span class="line"></span><br><span class="line"><span class="keyword">VOLUME</span> ["/Hexo/source", "/Hexo/themes", "/root/.ssh"]</span><br><span class="line"></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">80</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span> docker-entrypoint.sh /docker-entrypoint.sh</span><br><span class="line"><span class="keyword">COPY</span> inline.js /Hexo/node_modules/kramed/lib/rules/inline.js</span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span> ["/docker-entrypoint.sh"]</span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span> ['/bin/bash']</span><br></pre></td></tr></table></figure><h4 id="其中的-docker-entrypoint-sh-是"><a href="#其中的-docker-entrypoint-sh-是" class="headerlink" title="其中的 docker-entrypoint.sh 是"></a>其中的 docker-entrypoint.sh 是</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"></span><br><span class="line">echo "$@" | awk -F ' ' '&#123;print $1&#125;' | xargs git config --global user.name</span><br><span class="line"></span><br><span class="line">echo "$@" | awk -F ' ' '&#123;print $2&#125;' | xargs git config --global user.email</span><br><span class="line"></span><br><span class="line">if [ "$3" = 's' ] || [ "$3" = 'server' ]; then</span><br><span class="line">    set -- /usr/bin/hexo s -p 80</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ "$3" = 'd' ] || [ "$3" = 'deploy' ]; then</span><br><span class="line">    set -- /usr/bin/hexo cl &amp;&amp; /usr/bin/hexo d -g</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">exec "$@"</span><br></pre></td></tr></table></figure><p><strong>记得要加可执行的权限</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> chmod +x docker-entrypoint.sh</span><br></pre></td></tr></table></figure><h4 id="inline-js文件！"><a href="#inline-js文件！" class="headerlink" title="inline.js文件！"></a><strong>inline.js文件！</strong></h4><p>用于 Hexo 默认使用的 hexo-renderer-marked 引擎渲染网页，该引擎会把一些特殊的 markdown 符号转换为相应的 html 标签，比如在 markdown 语法中，下划线<code>_</code>代表斜体，会被渲染引擎处理为<code>&lt;em&gt;</code>标签。</p><p>因为类 Latex 格式书写的数学公式下划线<code>_</code>表示下标，有特殊的含义，如果被强制转换为<code>&lt;em&gt;</code>标签，那么 MathJax 引擎在渲染数学公式的时候就会出错。</p><p>类似的语义冲突的符号还包括<code>*</code>, <code>{</code>, <code>}</code>, <code>\\</code>等。</p><p>解决方法：</p><p>1.更换 hexo-renderer-kramed 引擎（见Dockerfile 17、18 行）</p><p>2.修改 {你的博客目录}/node_modules/kramed/lib/rules/inline.js 文件</p><p>修改第11行的 escape 变量的值</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span></span><br><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([`*\[\]()#$+\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure><p>修改第20行的 em 变量的值</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span></span><br><span class="line">em: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure><p>具体的 <a href="https://github.com/Dhqquan/docker-hexo/blob/master/inline.js" target="_blank" rel="noopener">inline.js 文件</a></p><h4 id="生成-Docker-Image"><a href="#生成-Docker-Image" class="headerlink" title="生成 Docker Image"></a>生成 Docker Image</h4><p>将<strong>Dockerfile</strong>、<strong>docker-entrypoint.sh</strong> 和 <strong>inline.js</strong> 放在同一个文件夹。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker build -t xxx/hexo .</span><br></pre></td></tr></table></figure><p>xxx/hexo 是你定义的 Image 名称，注意别忘了后面的 <code>.</code></p><h2 id="Docker-Hexo-Container"><a href="#Docker-Hexo-Container" class="headerlink" title="Docker-Hexo Container"></a>Docker-Hexo Container</h2><h3 id="hexo-server-Container"><a href="#hexo-server-Container" class="headerlink" title="hexo-server Container"></a>hexo-server Container</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -p 4000:80 --name hexo-server -d \</span><br><span class="line">-v &#123;你的博客文件夹路径&#125;/.ssh:/root/.ssh \</span><br><span class="line">-v &#123;你的博客文件夹路径&#125;/source:/Hexo/source \</span><br><span class="line">-v &#123;你的博客文件夹路径&#125;/themes:/Hexo/themes \</span><br><span class="line">-v &#123;你的博客文件夹路径&#125;/_config.yml:/Hexo/_config.yml \</span><br><span class="line">xxx/hexo &#123;你的 github username&#125; &#123;你的 github email&#125; server</span><br></pre></td></tr></table></figure><p>注意：</p><ul><li>{ 你的博客文件夹路径 } 即你新建的文件目录 eg：/Hexo</li><li>命令中的 xxx/hexo 即上一步的 Image 名称</li><li>命令中的 -v 即挂载命令</li></ul><p>Kitematic 中可以看到 hexo-server container 正在运行，点击 exec 可以进入 container 控制台。</p><h3 id="hexo-deploy-Container"><a href="#hexo-deploy-Container" class="headerlink" title="hexo-deploy Container"></a>hexo-deploy Container</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -p 4000:80 --name hexo-deploy -d \</span><br><span class="line">-v &#123;你的博客文件夹路径&#125;/.ssh:/root/.ssh \</span><br><span class="line">-v &#123;你的博客文件夹路径&#125;/source:/Hexo/source \</span><br><span class="line">-v &#123;你的博客文件夹路径&#125;/themes:/Hexo/themes \</span><br><span class="line">-v &#123;你的博客文件夹路径&#125;/_config.yml:/Hexo/_config.yml \</span><br><span class="line">xxx/hexo &#123;你的 github username&#125; &#123;你的 github email&#125; deploy</span><br></pre></td></tr></table></figure><p>hexo-deploy 将你的博客部署到 Github 上。</p><p>Github 地址：<a href="https://github.com/Dhqquan/docker-hexo" target="_blank" rel="noopener">https://github.com/Dhqquan/docker-hexo</a></p><h2 id="本章参考"><a href="#本章参考" class="headerlink" title="本章参考"></a>本章参考</h2><ol><li><a href="https://www.v2ex.com/t/281309" target="_blank" rel="noopener">接近于完美版的 docker-hexo 实现~</a></li><li><a href="https://fedoryx.github.io/%E5%88%A9%E7%94%A8-GitHub-Hexo-Docker-%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E7%8B%AC%E7%AB%8B%E5%8D%9A%E5%AE%A2-MAC%E7%AF%87/" target="_blank" rel="noopener">利用 GitHub + Hexo + Docker 快速构建独立博客 — MAC篇</a></li><li><a href="https://blog.csdn.net/wgshun616/article/details/81019687" target="_blank" rel="noopener">Hexo 的 Next 主题中渲染 MathJax 数学公式</a></li></ol><h1 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h1><h2 id="Hexo-简介"><a href="#Hexo-简介" class="headerlink" title="Hexo 简介"></a>Hexo 简介</h2><blockquote><p>快速、简介且高效的博客框架</p><p>超快速度、支持 Markdown、一键部署、丰富插件</p></blockquote><h2 id="Hexo-搭建准备"><a href="#Hexo-搭建准备" class="headerlink" title="Hexo 搭建准备"></a>Hexo 搭建准备</h2><ul><li>Node.js</li><li>Git</li></ul><p>这里已经通过 Docker 解决了环境</p><h2 id="Hexo配置文件"><a href="#Hexo配置文件" class="headerlink" title="Hexo配置文件"></a>Hexo配置文件</h2><ul><li>_config.yml 站点配置文件</li><li>themes 存放主题的文件夹</li><li>source 博客资源文件夹</li></ul><p>这些文件是我们需要在电脑上准备的</p><h2 id="Hexo-重要命令"><a href="#Hexo-重要命令" class="headerlink" title="Hexo 重要命令"></a>Hexo 重要命令</h2><p><strong>init</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hexo init [folder]</span><br></pre></td></tr></table></figure><p><strong>new</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hexo new [layout] &lt;title&gt;</span><br></pre></td></tr></table></figure><p>新建一篇文章。不过一般直接在 source 的 _post 下写</p><p><strong>generate</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hexo generate</span><br><span class="line"><span class="meta">$</span> hexo g</span><br></pre></td></tr></table></figure><p>生成静态文件</p><p><strong>server</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hexo server</span><br><span class="line"><span class="meta">$</span> hexo s</span><br></pre></td></tr></table></figure><p>启动 Hexo 服务器。默认情况下，访问网址是：<a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a></p><p><strong>deploy</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hexo deploy</span><br><span class="line"><span class="meta">$</span> hexo d</span><br></pre></td></tr></table></figure><p>部署网址到 Github</p><p><strong>clean</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hexo clean</span><br></pre></td></tr></table></figure><p>清除本地缓存</p><h2 id="本章参考-1"><a href="#本章参考-1" class="headerlink" title="本章参考"></a>本章参考</h2><ol><li><a href="https://hexo.io/zh-cn/index.html" target="_blank" rel="noopener">Hexo 官方文档 </a></li></ol><h1 id="后续使用"><a href="#后续使用" class="headerlink" title="后续使用"></a>后续使用</h1><h2 id="运行-Hexo-的初始版本"><a href="#运行-Hexo-的初始版本" class="headerlink" title="运行 Hexo 的初始版本"></a>运行 Hexo 的初始版本</h2>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker run -p 80:80 -d dhq/hexo &#123;你的 Github username&#125; &#123;你的 Github Email&#125; server</span><br></pre></td></tr></table></figure><h2 id="运行-Hexo-服务器，在本机查看效果"><a href="#运行-Hexo-服务器，在本机查看效果" class="headerlink" title="运行 Hexo 服务器，在本机查看效果"></a>运行 Hexo 服务器，在本机查看效果</h2>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker start hexo-server</span><br><span class="line"><span class="meta">$</span> docker exec -it hexo-server sh</span><br></pre></td></tr></table></figure><p>  就可以在 localhost:4000 查看具体的效果了。</p><h2 id="部署上-Github"><a href="#部署上-Github" class="headerlink" title="部署上 Github"></a>部署上 Github</h2>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker start hexo-deploy</span><br></pre></td></tr></table></figure><h2 id="每一个写文章的话，就自己手动创建到-source-post-目录下，就可以了。"><a href="#每一个写文章的话，就自己手动创建到-source-post-目录下，就可以了。" class="headerlink" title="每一个写文章的话，就自己手动创建到 source/_post 目录下，就可以了。"></a>每一个写文章的话，就自己手动创建到 source/_post 目录下，就可以了。</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文记录了在macOS上搭建Hexo博客的完整过程。之所以选择 Hexo + Docker + Github 搭建个人博客，是因为不想在 VPS 上搭建（写博客上传麻烦，之前搭过有被我弄坏了node环境），就准备在Docker上搭建。&lt;/p&gt;
&lt;h1 id=&quot;Docker&quot;&gt;&lt;a href=&quot;#Docker&quot; class=&quot;headerlink&quot; title=&quot;Docker&quot;&gt;&lt;/a&gt;Docker&lt;/h1&gt;&lt;h2 id=&quot;Docker安装&quot;&gt;&lt;a href=&quot;#Docker安装&quot; class=&quot;headerlink&quot; title=&quot;Docker安装&quot;&gt;&lt;/a&gt;Docker安装&lt;/h2&gt;&lt;h3 id=&quot;使用-Homebrew-安装&quot;&gt;&lt;a href=&quot;#使用-Homebrew-安装&quot; class=&quot;headerlink&quot; title=&quot;使用 Homebrew 安装&quot;&gt;&lt;/a&gt;使用 Homebrew 安装&lt;/h3&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; brew cask install docker&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;更多的安装方法：官方文档&quot;&gt;&lt;a href=&quot;#更多的安装方法：官方文档&quot; class=&quot;headerlink&quot; title=&quot;更多的安装方法：官方文档&quot;&gt;&lt;/a&gt;更多的安装方法：&lt;a href=&quot;https://yeasy.gitbooks.io/docker_practice/install/mac.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方文档&lt;/a&gt;&lt;/h3&gt;&lt;h2 id=&quot;Kitematic下载&quot;&gt;&lt;a href=&quot;#Kitematic下载&quot; class=&quot;headerlink&quot; title=&quot;Kitematic下载&quot;&gt;&lt;/a&gt;Kitematic下载&lt;/h2&gt;&lt;p&gt;Kitematic 是 Docker 官方的提供的可视化程序。&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; brew cask install kitematic&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="建站" scheme="http://blog.dhquan.cn/categories/%E5%BB%BA%E7%AB%99/"/>
    
    
      <category term="建站" scheme="http://blog.dhquan.cn/tags/%E5%BB%BA%E7%AB%99/"/>
    
      <category term="Docker" scheme="http://blog.dhquan.cn/tags/Docker/"/>
    
      <category term="Hexo" scheme="http://blog.dhquan.cn/tags/Hexo/"/>
    
  </entry>
  
</feed>
